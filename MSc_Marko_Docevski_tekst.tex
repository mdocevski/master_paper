\chapter{Вовед}

Is AI truly creative?
The relationship between maths and music is long established, with formulas underpinning all kinds of musical genres, from black metal to Indonesian gamelan. Ada Lovelace, the 19th century mathematician acknowledged as the first computer programmer, recognised the artistic potential of machines in the 1840s, suggesting that computers “might compose elaborate and scientific pieces of music of any degree of complexity”.
Nearly 200 years later, computers can do just that, thanks to the development of AI and the power of neural networks. “It’s a pared down representation of the way some scientists believe a human brain works,” says Ed Newton-Rex, co-founder of Jukedeck, an AI music composition tool. “By passing in lots of pieces of music, it can start to assess which notes should follow other notes. You end up with a system where you can choose a starting point and the algorithm can make a bunch of choices. Eventually, it builds an understanding of how to create a piece of music.”
According to Ashkhen Zakharyan, a spokesperson for Aiva, there’s an analogy between this process and the way humans compose – after all, we are also a sum of our influences.
“We learn from other artists,” she says, “and we embark on a trial-and-error process during which we don’t always get the notes right. But then we use our musical knowledge and musical ear to correct ourselves.
“Aiva is the same, but the process is reduced from a couple of years to a couple of hours.”
Newton-Rex is less convinced about this parallel. “Humans have very complex influences, like emotion, and memory,” he says.
“There’s a great deal that goes into human composition that no one in AI has got anywhere near. But that doesn’t mean it’s not powerful.”
Whether AI is truly creative or merely generating a kind of pastiche, the music it produces certainly sounds authentic. But our appreciation of the skill and ingenuity of computer programmers doesn’t equate to being entertained, according to Mark d’Inverno, Professor of Computer Science at Goldsmiths, University of London.
“People working in AI are understandably excited about generating something that would be called art if a human did it,” he says. “But I don’t see any signs that we want to experience computational work as art. Art is partly about unpicking the human experience of the person who created it.”

Генерирање на музика и други типови на мултимедиска содржина се мошне популарни теми на истражување во денешно време. Во академијата и популарните дискусии се води дебата за возможноста на оспосбување на компјутерски систем да покаже знаци на креативност, како што може да се види во \cite{Ghedini2015}. Едната страна на дебатата тврди дека компјутерите нема да можат, барем не во скоро време, да креират ништо уникатно и имагинативно бидејќи сите компјутерски системи за креирање на музика би биле зависеле од некој модел изграден врз колекција од композици направени од човек, или пак правила/граматики креирани од човек. Ваквите системи би работеле со некаков произволно апстрактен и комплексен систем на комбинации и рекомбинации за креирање на нови дела. Ова може да се смета како клучен лимитирачки фактор за израз на имагинација и инвентивност. Другата страна на дебатата ова не го смета како ограничување, туку како неопходно зло, или како еволутивен чекор во развивање на компјутерска креативност, исто како што е и дел од развојот на секој човек. Најголемиот дел од луѓето стапуваат во контакт со музиката долго време пред да започнат самите да допринесат кон целокупното човечко творештво. Така во делата на секој човек може да се пронајдат влијанија од другите автори со чии дела имаат стапено во контакт. Имитацијата е основен елеменет во процесот на учење, како и форма на одавање почит. Врз основа на ова, пропонентите на компјутерската креативност тврдат дека ваквите огрничувања се во најлош случај само еволутивен чекор. (МОЖЕБИ ТРЕБА ДА СЕ РЕФОРМУЛИРА ПОУБАВО)

Андреј Карпати со статија „Неразумната ефективност на рекурентните невронски мрежи“ \cite{AndrejKarpathy2015} значително го зголеми интересот на полето на машинско учење, поточно на невронските мрежи и нивните рекурентните варијанти. Во статијата е прикажан генеративен јазичен модел на ниво на буква, составен од длабока невронска мрежа изградена од рекурентни ќелии (рекурентна невронска мрежа), истрениран на повеќе колекции на текстови од различен карактер (есеи од Пол Греам, драми од Шекспир, XML од Википедиа, LaTeX текстови и C\\C++ изворен код). Моделот ги учи текстовите буква по буква, и резултатот го генерира на истиот начин. Иако не му се додаваат никакви информации за структурата на текстовите, на јазикот, граматички правила или форматирање, тој успева да генерира резултати кои се конформираат во голема мера кон изворниот формат. Самиот креира ад-хок правила за конструкција на сложени зборови, генерирајки и сложени зборови што ги нема во изворните текстови, учи употреба на сврзници, честички, негации, сложени реченици иако не секогаш се поврзани просите реченици и сл. Во случајот каде што учи технички документи и изворен код ги прати и долгорочните зависности на отворање и соодветно затворења на загради и маркери кои често се простираат на растојание од повеќе стотини букви низ текстот. Успешноста на овој модел има инспирирано огромен број на истражувачи и луѓе од индустријата да се обидат да креираат генеративни модели со користење на рекурентни невронски мрежи со релативно едноставни архитектури. Дел од трудовите што ќе ги претставиме во наредното поглавје, вклучувајки го и нашиот се поттикнати од успехот на овој експеримент.

Покрај желбадата да придонесеме кон филозофксата дискусија за компјутерската креативност, увидовме и повеќе можности за практично искористување на систем за генеирање на музика, вклучувајки:
\begin{itemize}
\item Музика за видео игри, т.е. процедурално генерирана музика, кадешто музката ќе се адаптира на атмосферата и претходно дефинирани параметри, темпо и сл.
\item Музика за вежбање. Во принцип генерирање на музика што треба да следи предефинирани рутини за вежбање и ќе помага во одржување на темпо и енергетско ниво и ќе стимулира. Додатно ритамот и темпото на музиката може да бидат и под влијание на виталните знаци на корисникот, примарно пулс и дишење.
\item Компјутерски помогнато компонирање на музика. Човечки композитор користи апликација која му пружа можност за дополнување на музика, варијација на постоечка музика според параметри и тн.
\end{itemize}

Уште пред да започнеме со работа знаевме дека било каков генеративен систем со модели за машинско учење има огромен потенцијал за тесно грло бидејќи не постои начин за квалитативно оценување на излезот од таков моделот, т.е. не постои целна функција која што може да се оптимизира во процесот на учење. Во трудовите што ги проучивме, кои ќе ги разгледаме во поглавје {СТАВИ ЛИНК ДО ПОГЛАВЈЕ}, најчест пристап е рачна евалуација на резултатите од страна на човек, којшто во принцип е музички образован. Ваквото тесно грло не само што елиминира гаранција за квалитет на резултатите, туку и го ограничува изборот на алгоритми за машинско учење. 

Досегашните обиди за генерирање на музика може да се поделат на две категории: генерирање на пишана музика и генерирање на аудио сигнал. Пристапите за пишана музика значително варираат во комплексноста на проблемот што пробуваат да го решат, тргнувајќи од генеирање на „12 bar blues“ [ЦИТИРАЈ ЕК] блуз во 12 такти, до народна музика [ЦИТИРАЈ ФОЛК РНН], па се до Бахови корали во 4 гласа [ЦИТИРАЈ БАХБОТ И ОСТАЛИ]. Во другата категорија досегашните обиди [цитирај ВЕЈВНЕТ] се ограничени до учење на звучен сигнал од еден инструмент со ограничена должина, или работат со голема улога на човечки композитор [ЦИТИРАЈ СОНИ ЦСЛ ДЕДИС КАР]. Ние одлучувме да се зафатиме со проблемот на генерирање на пишана музика, поточно со полифона музика во еден глас. Додатни структурни ограничувања не сакавме да ставиме од самиот почеток, освен музиката да е од ист или сличен жанр и одеден или мал и ограничен број на автори.

Во трудот ќе ја прикажеме нашата работа на полето на генеративни музички модели која вклучува собирање и анализа на две посебни податочни множества, првото од дела за класична гитара во MIDI* формат, другото рачно изградено множество добиено со конверзија на GuitarPro табулатури за гитара како и повеќе експерименти со различни архитектури за машинско учење. Ги искористивме следниве архитектури во различни конфигурации: повеќеслојна LSTM* рекурентна невронска мрежа, повеќеслојна LSTM рекурентна невронска мрежа во комбинација со целосно поврзани слоеви и Encoder-Decoder архитектура изградена од LSTM слоеви. Ќе дадеме и толкување на добиените резултати, како и можностите за подобрување на системот што ги согледавме.

Трудот е организиран во следните поглавја: во поглавјето [ЦИТИРАЈ МОТИВАЦИЈА И ДЕФИНИЦИЈА НА ПРОБЛЕМОТ] ќе ја дадеме мотивацијата за превземање на истражувањето и ќе го дадеме дефиниција на задачата што сакаме да ја завршиме, во поглавјето [ЦИТИРАЈ ПРЕГЛЕД] даваме преглед на досегашните решенија и ќе ги споредиме со нашето решение, во поглавјето [ЦИТИРАЈ ДАТАСЕТ] ги опишуваме и даваме анализа на податочните множества што ги собравме, во под-поглавјето [ЦИТИРАЈ РЕПРЕЗЕНТАЦИЈА] ќе ги опишеме адаптациите на податочното множество за неговор користење со модели за машинско учење, во поглавјето [ЦИТИРАЈ АРХИТЕКТУРИ] ќе ги опишеме архитектурите за машинско учење што ги искористивме во експериментите, во поглавјето [ЦИТИРАЈ ЕКСПЕРИМЕНТИ] ќе опишеме експериментите што ги извршивме, во поглавјето [ЦИТИРАЈ РЕЗУЛТАТИТ] ќе ги опишеме и толкуваме резултатите од експериментите и во последното [ЦИТИРАЈ ЗАКЛУЧОК] поглавје ќе ги дадеме заклучоците од нашата работа опишана во овој труд, како и ќе ги забележиме можностите за подобрување кои ги увидовме.

\chapter{Преглед на литературата}

Еден од раните обид за „алгоритамско“ креирање на музика е познат како „Музички игри со коцки“, бил популарен во XVIII век. Првата забележана композиција создадена на овој начин е „Секогаш спремен минует и поноњески композитор“ (гер. Der allezeit fertige Menuetten- und Polonaisencomponist) од Јохан Филип Кирнбергер уште во 1757год. Најпопуларни биле игрите напишани од страна на Хајдн и Моцарт, поради што може да се пронајдат вакви игри и под името Моцартови коцки. Ваквуте игри најчесто се состојат од таблица од музички исечоци, во големина од еден до неколку такта, кои се напишани така да немаат остри рабови за да може лесно да се надоврзуваат. Играчите на играта фрлаат коцки или случајно избираат број, па според тоа избираат исечок од табелата според одредена шема која иде со табелта, и избраниот исечок го прилепуваат на композицијата. Играта трае се додека играчот не одлучи дека има доволно долга композиција. Квалитетот на добиената композиција зависи најмногу од квалитетот на исечоците. Бројот на можни исечоци мора да биде релативно мал за да може играчите да може практично да играат, а да не прелистуваат стотици страници секој пат кога ќе свртат коцка. Исечоците за да можат да бидат лесно поврзливи од двете страни практично треба да имаа почеток, средина и крај, т.е. да бидам самостојни микро-композиции. Поради сите овие ограничувања, Музичките игри со коцки повеќе се игра отколку практичен начин за алгоритамски пристап за компонирање музика. Доклку се решат сите ограничувања играта би добила многу поголем капацитет за креативност, меѓутоа би била непрактична за човечка употреба. Тука влегуваат во игра компутерски имплементации на играта. Скоро сите пристапи кои ги истражив може да се апстрахираат како играта врз основа на 2 клучни точки:
\begin{itemize}
    \item Дефиниција на исечок / Освноен елемент на композиција 
    \item Избор на исечок / Начин на прилепување на исечоците
\end{itemize}
Пристапите кои ги разгледав може се подделат во две епохи пред длабоко учење и со длабоко учење. Пред длабоко учење сите пристапи се доста блиски до игрите со коцки, поголем дел бараат само начин на заменување на процесот на избор на исечок со: правила и граматики, експертски системи, генетски алгоритми. Пристапите со длабоко учење се повеќе се оддалечуваат од игрите, но сепак суштински го вршат истото. 

\section{Граматички / експертски системи} 

Д. Коуп во трудот \cite{Cope1991} ја опишува првата компјутерска имплементација на музичките игри со коцки. Коцките се заменети со стохастички процес. Исечоците се претставени со шаблони, кои ги добиваат со делење на корпус од пишана музика, во должини од еден до два такта. Сите шаблони се анализираат спооред рачно пишани правила базирани на музичка теорија и содржат „потпис“ на авторот и се чуваат во лексикон. Случајниот процес бира од кандидат шаблони од лексиконот коишто се компатибилни со последниот избран шаблон. Компатибилоста ја одредуваат со анализа на тон и должина на ноти и со споредба на шаблони, којашто ја врши врз основа на релативно движење на тоновите и должините.

Импелемнтација на играта со генетски алгоритам може да се види во \cite{Biles1994}. Тука исечоците се претставени со хромозоми на алгоритмот. Музиката е во строг 4/4 ритам, секој хромозом е еден такт составен од 8 настани во должина од 1/8, при што настан може да бидат нова нота, задржување на стара нота и пауза. Квалитет на секој од хромозомите при процесот на еволуција се одредува рачно од страна на човечки ментор, кој венсува вредност со помош на тастатура. Ова е очигледна болна точка за практичноста на алгоритамот, што го прави алгоритамот крајно непрактичен, потребно е многу време за вешт ментор да ги преслушува кандидатите и да ги оценува. 

Во трудот \cite{Zils2001} авторите опишуваат експертски систем за креирање на мозаик од музички сегменти (исечоци). Сегментите се опишани со многжество на дескриптори. Дефинираат два вида на правила, сегментни, т.е. правила кои што се евалуираат на ниво на сегмент, и секвенцни, т.е. глобални правила, коишто се евалуираат на целата секвенца. Дел од правилата се преддефинирани, а дел корисникот на алгоритмот ги внесува рачно. Доклку сака корисникот може и да избере готова песна врз основа на која ќе се екстрахираат правила врз основа на кои ќе биде креирана нова песна, во суштина имитирајќи ја оригиналната песна од високо ниво. Бидејќи правилата не се секогаш се во согласување, авторите имаат и дефинирано постапка за евалуација на важноста на правилата врз основа на тежини, т.е. функција на чинење која ја минимизираат при процесот на генерирање.

Пристапот опишан во \cite{GarciaSalas2011} може наједонставно да се објасни како n-gram модел или модел со Маркови синџири. Системот се состои од дел за учење и дел за копонирање. Делот за учење генерира правила врз основа на податочното множество, а делот за компонирање генерира нова музика врз основа на правилата и има повратна врска кон делот за учење. Правилата се изразени преку 3 матрици: матрица на времетраења на нотите, локална матрица на фрекфенции и кумулативна матрица на фрекфенции. Овие матрици се пополнуваат според n-gram фрекфенции на појавување во податочното множество. Алгоритамот за генерирање пресметува матрица на веројатност на појавување како функција од претходно споменатите матрици. Генерирањето се врши со стохастичен процес врз основа на матрицата на веројатности. 

Уште еден систем за градење на музички мозаици може да се види во \cite{Schwarz2006}, каде авторите имаат изградено корпус од кратки исечоци, секој опишан со одредено множество на дескриптори, наголем дел добиени со математички трансформации на аудио сигналот, пр. брзи фуриеви трансформации, гаворови бранови, хистограми и сл. Системот има алгоритам за избирање на исечоци, кои последователно ги лепи едни за други за време на процесот на генерирање. Алгоритамот е базиран на човечки зададени правила или имитација на постоечки песни, кадешто алгоритамо само ги бира исечоците кои ги задоволуваат критериумите базирани на десктрипторите, без да води сметка за колку добро се сложуваат меѓусебе, и врши благо измазнување на краевиењ помеѓу исечоците.

\section{Алгоритми со длабоко учење} 

Голем број на алгоритми и техники измислени дури од самите почетоци на истражување на полето на машинско учење, како што се модели со невронски мрежи, машини со носечки вектори, дрва на одлука и сл., во минатото не покажува многу добри резултати во однос рачно изградени експертски модели. Најголемо ограничување беа пресметковната моќ и меморискиот капацитет на машините на коишто се извршуваа истите, влијаејќи на големината и комплексноста на моделите за машинско учење како и количината на податоци што може да се обработи при учење на истите. Меѓутоа во последните десетина години започнаа да се користат графичките акцелератори (попознати како графички картички), дотогаш највеќе користени за видео игри, за општа намена (GPGPU - General Purpouse GPU). Се појавија на сцена две библиотеки CUDA од NVIDIA Corp. и OpenCL - слободен софтвер развиен од заедницата на корисници, кои овозможуваат користење на графичките акцелератори за секакви пресметковни намени, што се покажа многу корисно за развивање на модели за машинско учење. Оттокаш се појави и експлозивен интерес за изстражување на секакви теми и решавање на секакви проблеми со користење машинско учење, вклучувајќи и модели за компјутерско генерирање на музика. Самото зголемување на пресметковна моќ овозможува создавање на многу подлабок, пософистициран и поапстрактен систем од тоа што го нудеа претходно споменатите техники. Во продолжение следуа преглед на повеќе пристапи за генерирање на музика со длабоко учење, кое претставува подмножество на техники за машинско учење со користење на длабоки невронски мрежи.

Првиот документиран обид за користење на невронски мрежи за генерирање на музика е од страна на Д. Ек во трудот \cite{Eck2002}. Целта е тренирање на модел кој ќе генерира блуз во 12 такта. Музиката се состои од 2 оделни компоненти, секвенца на акорди кои го одредуваат ритамот и ја даваат основната структура на музиката и мелодиска линија. Моделот има две групи од по 4 пара LSTM (Long Short Term Memory) ќелии (рекурентни неврони), од кои едната е задолжена за учење на мелодијата, а друга за акордната секвенца. Музиката ја преставуваат во тн. репрезентација во пиано лента, која е дво димензионална бинарна матрица во која едната оска (подолгата) го претставува времето, а другата оска го преставува множеството од можни тонови. Музиката има строга структура, 12 такта, 4/4 ритам и времето е подделено на 1/8 ноти. Моделот го испробале на два експерименти, во првиот тој ја учи само секвенцата на акорди, а во вториот и акордите и мелодијата. Тренирањето е извршено со cross-entropy фунција на загува, со сигмоидална функција на активација. Делот за учење на акорди ја споделува својата внатрешна состојба со делот за учење на мелодијата. Авторот смета дека системот генерира добра музика, но сепак проблемот што пробува да го реши е хипер фокусиран и ограничен. Трудот \cite{Eck2008} претставува надоградување на претходно опишаниот систем. Го нема веќе фокусот на блуз во 12 такти, туку работат со податочно множество од поразлини MIDI датотеки, зголемена е комплексноста на моделот, со многу повеќе ќелии и повеќе слоеви од истите. Додатно на моделот му прикажуваат и временски дилатирани копии од податоците, на семантички важни растојание. Ова доаѓа од хипотезата дека во музиката значајни настани се случуваат на метрички важни места како што се почетокот и крајот на тактовите, како и на моменти зависни од ритамо (пример во македонскиот 7/8 ритам ен-два, ен-два, ен-два-три, има значајни транзиции на секое „ен“). Ова го постигнуваат со проширување на пиано лентата да при секој чекор на моделот му се прикаже лентата од тековниата музика, и од музиката од претходно зададени интервали, како на пр. пред 1, 4 и 16 такта толно на истиот удар во тактовите. Покрај LSTM слоеви моделот има и паралелен целосно поврзан слој неврони којшто треба да помогне во учење на локални зависности меѓу нотите. Мрежата ги учи песните како една секвенца, со што грешката при учење се ресетира акумулираната грешка на граница меѓу песните. Нотите се ограничени помеѓу C3 и C5 и сите песни се транспонирани во ист клуч. Користени се ирски народни песни во 4/4 ритам. Моделот генерира нови песни со предвидување на следната нота откако ќе се припреми претходно со исечок од постоечка песна.

Во трудот \cite{Sturm2016} авторите користат техники од обработка на природни јазици за генерирање на музика. Караткеристична е употребата на форматот за музика познат како ABC. Форматот е текстуален, наменет примарно за запишување на изворна музика на едноставен и лесно разбирлив начин за луѓето, содржи мелодиска линија запишана со основни тонови и пропратен тескт. Музиката пишана во овој формат е опишана на високо ниво и доста стилски хомогена. Сите овие фактори ги прават форматот и податочното множество одлични за обработка со техники за обработка на природни јазици. Со користење на многу едноставен јазичен модел на ниво на буква и едноставна архитектура од 3 слоја од 512 LSTM неврони, со софтмакс активациски слој авторите добиле многу позитивни резултати. Во трудот прикажуаат и додатно подобрување на системот со повеќе чекори на предпроцесирање на текстот, користејки малку семантичко знаење, го трансформираат текстот од низа букви во низа уникатни и музички значајни токени врз основа на музичката функција на буквите/симболите. Додатно извршиле и процес на селекција и стандардизација на множеството песни за да ги исфрлат песните коишто се: премногу кратки, недволно информативни, конфузни или двосмислени; и го извршиле транскрипција на сите песни во ист клуч. Моделот е трениран со множество од над 23000 песни, во мини-бечови од 50 елементи, во 100 епохи. Генерирањето се врши со итеративна постапка, започнувајќи со посебен симбол за старт. Имаат извршено додатен експеримент во кој го стартуваат моделот со подолг извадок од песни од одредени автори за да ги проверат можностите на моделот да генерализира стилски карактеристки, и авторите тврдат дека моделот е способен за тоа.

Врз основа на техники за обработка на природни јазици може да се направат и модели на ниво на збор. Ваков пристап си има свои ограничувања бидејќи се зголему драстично бројот на основни елементи, има многу повеќе зборови одошто букви и симобили во јазициите, и нивните фрекфенции на појавување опаѓаат драстично. За да се надмине ова се користат различни техники за намалување на бројот на варијабли во системот, т.е. намалување на бројот на зборови, групирање на зборовите според функција, споделување на веројантности помеѓу зборовите и трансформации во помалку димензионален простор. Токму ваков пристап може да се види во \cite{Bretan2016}, каде што авторите третираат исечоци од 1 до 4 такта како зборови. Бидејќи има огромен број на можни варијации на таквите исечоци истите ги смалуваат во помалку димензионален простор користејќи ја техниката на вградување во латентен простор (анг. latent space embedding). Вградувањето го вршат врз основа на сет од дескриптори со кои ги опишуваат исечоците. Со ова се тобива структура налик на хеш табела, само со повеќе можни резултати при процесот на декодирање од елемент во табела во исечок. Тренираат два модела, еден што учи секвенци од репрезентации на зборовите, а друг што ги учи песните на ниво на буква и е наменет за решавање на проблемот на декодирање, т.е. ги гледа работиве помеѓу тековната секвенца и сите потенцијални декодирани исечоци, и помага во изворот на најсоодветниот кандидат. Намената на моделот на ниво на збор е да учи музички зависности на подолг временски период, во обид да се научи структура на цела песна, додека другиот модел ги учи локалните зависности помеѓу нотите. На крај извршиле субјективна евалуација од страна на 32 музички експеримент, коишто ги рангирале резултатите од повеќе инстанци на моделот со различн параметри.

Коралите на Јохан Себастијан Бах претставуваат интересен проблем за моделирање. Се стостојат од 4 гласна полифонија, алто, тенор, сопрано и бас монофонични мелодиски линии. Може да се моделираат на сличен начин како акордите, со тоа што зависностите помеѓу гласовите не е иста како меѓу нотите во акордите. Исто така уникатните комбинации од ноти по гласови поретко се јавуваат одошто кај акордите, бидејќи акордите се стандардни конструктивни елементи во музиката. Ова податочно множество е главен фокус на трудовите \cite{Liang2017, Hadjeres2016}. Во \cite{Liang2017} е искористен релативно еднсотавна архитектура, составена од повеќе рекурентни LSTM слоеви, со embedding слој после влезниот инспириран од word2vec\cite{Herremans2017}, со едноставен софтмакс активациски слој. Бидејќи ритамот во целото податочно множество е 4/4, музиката е поделена на еднакво долги рамки со времетраење 1/16-ка, во која 4те гласа се претставени со субсеквенца од поединечни ноти, а рамките меѓусебно се поделени со посебен симбол за разграничување. Со тоа се продолжува ја намалуваат комплексноста на проблемот за предвидување од $O(128^4)$ на $O(128)$, но ја издолжуваат секвенцата со фактор 5. Со алгоритам оптимизација на хиперпараметри GridSearch ги оптимизирале следните парамерти: број на рекурентни слоеви, големина на рекурентните слоеви, големина на embedding слојот, должина на секвенца што се користи за TBPT (анг. Truncated Back Propagation Through Time, Скратена повратна инфомација назад низ времето), процент за dropout (техника за нормализација, толкувано губиток на излезна информација). Во трудот \cite{Hadjeres2016} авторите користат посебни модели за секој од гласовите, и на крај ги сумираат нивните резултати. Ова влече одредена независност помеѓу гласовите, што во реалност секако не е точна, гласовите се функционално зависни помеѓусебе. За секој глас имаа модел составен од две рекурентни длабоки невронски мрежи, од кои едната го обработува времето наназад, а другата нанапред, и една целосно поврзана невронска мрежа која ги учи истровремено појавените ноти. Овие три подмрежи работат во паралела за секој временски чекор, и излезите од сите три се користат како влез во финална општа невронска мрежа која ги влече заклучоците за секој временски чекор. За генерирање користат псевдо-Гибсово семплирање (варијанта на Монте карло Маркови синџири), со кое влечат примероци од добиените излези од 4те подмодели за секој глас. Податочното множество го имаат транспонирано во сите можни клучеви наместо стандардизирање кон еден клуч. И во двата труда се прикажани процеси за субјективна оцена на резултатите со онлајн тестови каде што на корисниците им се пушта музички исечок и треба да одлучат дали е генерирана музика или оригинална композиција на Бах. Во двата случаја најпозитивни резултати покажале кога моделите ги користеле за рехармонизација на мелодија врз основа на 2 или 3 постоечки гласа.

Во системот наречен „Производ на експерти“ предложен во \cite{Johnson2017} се користат два прости јазични модели во тандем, тренирани на истото податочно множество од џез песни, со различен перпектива врз истото. Двата модела се едноставни рекурентни длабоки невронски мрежи, од кои едниот ги гледа песните како секвенца од релативно движење на мелодијата, т.е. при секој чекор ја гледа абсолутната разлика во тонот меѓу последователни ноти, додека другиот ја следи хармониската улога на нотата во тековниот акорд од прогесијата на акорди. Двата модели при секој чекор моделираат веројатностна дистрибуција и може едноставно да се тренираат со оптимизациски алгоритми за намалување на крос-ентропија, а финалниот резултат е параметризирана сума од двете. Тренирањето и генерирањето на песните следи стандардна итеративна процедура.

Користењето на јазични модели на ниво на бука овозможува едноставно моделирање на монофона музика, или музика која во секој момент има највеќе една активна нова. При моделирање на акорди со пиано лента се појавува проблем на енумерирање сите можни конфигурации на ноти што може да се појават, пр. за гитара има грубо $2*24^6$ можни конфигурации на ноти. Еден пристап за намалување на проблемите што доаѓаат од зголемена димензионалност може да се види во \cite{Boulanger-Lewandowski2012, Boulanger-Lewandowski2014, Goel2014}, каде што авторите користат хибридна архитектура, изграден од рекурентни невронски слоеви во различни комбинации со енергетски модели. Идејата е да се искористат особеностите на енергетските модели, како што се RBM (Restricted Boltzmann Machines - Ограничени Болзманови Машини) и NADE (Neural Autoregressive Distribution Estimator), да моделираат веројатностни дистрибуции, кон моделирање на истовремено појавување на нотите во акорди. Од \cite{Boulanger-Lewandowski2012, Boulanger-Lewandowski2014} произлегуваат моделите: RTRBM (Recurrent Temporal RBM - Рекурентна Темпорална Ограничена Болзманова Машина), RNN-RBM, RNN-NADE, од кои RNN-NADE се покажал како наједноставен за тренирање и со најдобри генеративни резултати и најекспресивен, иако од енергетските модели NADE има релативно помала експресивна моќ. Моделот може да се разбере како низа од условени енергетски слоеви, по еден за секој временски чекор што го обработува мрежата, со излез во детерминистички рекурентен невронски слој. Наголем проблем на овој тип на модели се покажала осетливоста на иницијална состојба, што многу ја потенцира важноста на пред-тренирање при нивна употреба. Моделот се покажал успешен во моделирање на локални зависности, хармониски правила и кратки мелодиски линии, меѓутоа не може да моделира долговременски зависности. Во \cite{Goel2014} авторите користат пософистицирана варијанта на RNN-RBM наречена RNN-DBN, во која секвенците од RBM слоеви се заменети со секвенци од длабоки подмрежи изградени од повеќе хиерахиски RBM слоеви. Не користат никакви техники за иницијализација на енергетските слоеви како во \cite{Boulanger-Lewandowski2012, Boulanger-Lewandowski2014, Goel2014} ниту за оптимизирање на работата на моделот.

Јазичните модели на ниво на буква може да се прошират на повеќе начини. Во трудот \cite{Tikhonov2017} авторите предложува збогатување на самите букви и користење на embedding или врадување на самите букви и на секвенци со одредена должина. Се ограничуваат со генерирање на монофонична музика, користејќи огромна колекција на MIDI датотеки, со низа чекори за филтрирање, одстранување на сувишни информации, транспозиции и уедначувања добиле податочно множество од повеќе од 15000 песни составени само од мелодиска линија. Секоја нота од мелодиската секвенца е претставена со embedding-зи за: висината, октавата и времетраењето како и со мета-информации извлечени од MIDI датотеката. Ваквата репрезентација ја кодираат во 1 од N бинарен вектор на влез, и на излезот од системот користат 1 од N софтмакс активација. Средниот дел од архитектурата се состои од енкодер и декодер длабока рекурентна подмрежа. Двете подмрежи се обратно свртени пирамиди кои се сретнуваа со врвовите, т.е. во енкодер во секој нареден слој се намалува бројот на неврони се додека да се стигне до крајниот слој наречен тесно грлчо. Така секој нареден слој ги компресира податоците во густа репрезентација. Во декодерот се случува обратното, секој нареден слој има повеќе неврони и тој го открива значењето на густата реперзенатиција. Излезот од декодерот оди во активацискиот слој. Песните се делат на подсеквенци и моделот ги учи нивните густи репрезентации и како да ги добие секвенците од нивните репрезентации. Архитектурата ја нарекле VRASH - Variational Recurrent Autoencoder Supported By History (Варијациски аутоенкодер поддржан од историја) и претставува обид за додавање на варијациски баесов шум кон јазичен модел со користење на рекурентен аутоенкодер.

Во трудот \cite{Oord2016} дефинираат архитектура за обработка и учење на аудио сигнал со користење на конволуциски невронски мрежи, наречен WaveNet. Новоста во архитектурата се додадените временски дилатации и условеност на мрежата. Дилатациите функционираат на тој начин што временски задоцнети копии од податоците им се предаваат на повисоките слоеви во одредени интервали. Пр. влезниот слој обработува податоци од моменти: $t$, $t-1$, $t-2$ и $t-4$, следниот од $t$, $t-1$ и $t-2$, а последниот само од $t$ и $t-1$. Мрежата се улосував со пресметување на условни веројатности при извршување на конволуциите врз основа на класа на звук што се обработува, нпр. кој го изговара звучниот сегмент, на кој инструмент е отсвирен и тн. Архитектурата се покажала со ограничено рецептивно поле од околу $1/4$ од секунда, па учи мошне кратки секвенци, но тоа го прави многу добро. Најголем недостаток на самата архитектура е многу големата пресметковна цена, моделот го тренираат со недели на компјутерски кластер и му треба повеќе од час да генерира една секунда аудио сигнал, а времето расте со бројот на елементи во архитектурата (неврони по слој, број на слоеви).

Во трудот \cite{Mehri2016} авторите предложуваат хиерархија од модели за учење на музка од аудио сигнал. Најниското ниво обработува рамки со големи од еден примерок, а секое нагорно ниво обработува се поголеми рамки, или подолги временски периоди, без преклоп. Сите нивоа освен најниското се длабоки рекурентни невронски мрежи, а најниското ниво е повеќеслојна перцептрон мрежа. Погорните слоеви влијајат на подолните, така што нивните излези се користат како влез на пониско ниво, слично на деконволуција. Најнискиот слој генерира дистрибуција врз просторот на можни примероци во наредниот временски момент. Целата хиерархија се тренира во целост крај-до-крај. Извршиле AB тест за субјектина оцена, споредувајќи го дво и трослојна варијанта на моделот со нивна квази-имплементација на WaveNet \cite{Oord2016} и едноставен рекурентен модел, од кои тврдат дека 3 слојниот модел е префериран од страна на оценувачите.



\cite{Yang2017} MidiNet
Following this light, we investigate in this paper a novel CNN-based model for symbolic-domain generation, focus- ing on melody generation. 1 Instead of creating a melody sequence continuously, we propose to generate melodies one bar (measure) after another, in a successive manner. This allows us to employ convolutions on a 2-D matrix representing the presence of notes over different time steps in a bar. We can have such a score-like representation for each bar for either a real or a generated MIDI.Moreover, to emulate creativity [23] and encourage diverse generation result, we use random noises as input to our generator CNN. The goal of the generator is to trans- form random noises into the aforementioned 2-D score- like representation, that “appears” to be from real MIDI. This transformation is achieved by a special convolution operator called transposed convolution [8]. Meanwhile, we learn a discriminator CNN that takes as input a 2-D score- like representation and predicts whether it is from a real or a generated MIDI, thereby informing the generator how to appear to be real. This amounts to a generative adversarial network (GAN) [11–13,24,27], which learns the generator and discriminator iteratively under the concept of minimax two-player game theory. This GAN alone does not take into account the temporal dependencies across different bars. To address this issue, we propose a novel conditional mechanism to use music from the previous bars to condition the generation of the present bar. This is achieved by learning another CNN model, which we call the conditioner CNN, to incorporate information from previous bars to intermediate layers of the generator CNN. This way, our model can “look back” without a recurrent unit as used in RNNs. Like RNNs, our model can generate music of arbitrary number of bars.
For simplicity, we filtered out MIDI tabs that contain chords other than the 24 basic chord triads (12 major and 12 minor chords). Next, we segmented the remaining tabs every 8 bars, and then pre-processed the melody channel and the chord channel separately, as described below. For melodies, we fixed the smallest note unit to be the sixteenth note, makingw = 16. Specifically, we prolonged notes which have a pause note after them. If the first note of a bar is a pause, we extended the second note to have it played while the bar begins. There are other exceptions such as triplets and shorter notes (e.g. 32nd notes), but we chose to exclude them in this implementation. More- over, for simplicity, we shifted all the melodies into two oc- taves, from C4 to B5, and neglected the velocity of the note events. Although our melodies would use only 24 possible notes after these preprocessing steps, we considered all the 128 MIDI notes (i.e. from C0 to G10) in our symbolic representation. In doing so, we can detect model collaps- ing [12] more easily, by checking whether the model gen- erates notes outside these octaves. As there are no pauses in our data after preprocessing, we do not need a dimension for silence. Therefore, h = 128. For chords, instead of using a 24-dimensional one-hot vector, we found it more efficient to use a chord representa- tion that has only 13 dimensions— the first 12 dimensions for marking the key, and the last for the chord type (i.e. major or minor), as illustrated in Table 2. We pruned the chords such that there is only one chord per bar. After these preprocessing steps, we were left with 526 MIDI tabs (i.e. 4,208 bars). 5 For data augmentation, we circularly shifted the melodies and chords to any of the 12 keys in equal temperament, leading to a final dataset of 50,496 bars of melody and chord pairs for training.
Because we use random noises as inputs to our generator, our model can generate melodies from scratch, i.e. without any other prior information. However, due to the conditioner CNN, our model has the capacity to exploit whatever prior knowledge that is available and can be rep- resented as a matrix. For example, our model can generate music by following a chord progression, or by following a few starting notes (i.e. a priming melody). Given the same priming melody, our model can generate different results each time, again due to the random input.
Symbolic Representation for Convolution
Our model uses a symbolic representation of music in fixed time length, by dividing a MIDI file into bars. The note events of a MIDI channel can be represented by an h- by-w real-valued matrix X, where h denotes the number of MIDI notes we consider, possibly including one more dimension for representing silence, and w represents the number of time steps we use in a bar. For melody gener- ation, there is at most one active note per time step. We use a binary matrixX ∈ {0, 1}h×w if we omit the velocity (volume) of the note events. We use multiple matrices per bar if we want to generate multi-track music. In this representation, we may not be able to easily distinguish between a long note and two short repeating notes (i.e. consecutive notes with the same pitch). Future exten- sions can be done to emphasize the note onsets.
Generator CNN and Discriminator CNN
The core of MidiNet is a modified deep convolutional gen- erative adversarial network (DCGAN) [24], which aims at learning a discriminatorD to distinguish between real (au- thentic) and generated (artificial) data, and a generator G that “fools” the discriminator. Our discriminator is a typical CNN with a few convolution layers, followed by fully-connected layers. These lay- ers are optimized with a cross-entropy loss function, such that the output of D is close to 1 for real data (i.e. X) and 0 for those generated (i.e. G(z)). We use a sigmoid neuron at the output layer of D so its output is in [0,1].
Conditioner CNN
We propose to achieve this by using a conditioner CNN
that can be viewed as a reverse of the generator CNN. As the blue blocks in Figure 1 illustrates, the conditionerCNN uses a few convolution layers to process the input h-by-w conditional matrix. The conditioner and generator CNNs use exactly the same filter shapes in their convolution lay- ers, so that the outputs of their convolution layers have “compatible” shapes. In this way, we can concatenate the output of a convolution layer of the conditioner CNN to the input of a corresponding transposed convolution layer of the generator CNN, to influence the generation process. In the training stage, the conditioner and generator CNNs are trained simultaneously, by sharing the same gradients.
Tunning for Creativity
We propose two methods to control the trade-off between creativity and discipline of MidiNet. The first method is to manipulate the effect of the conditions by using them only in part of the intermediate transposed convolution layers of G, to give G more freedom from the imposed conditions. The second method capitalizes the effect of the fea- ture matching technique [27]: we can increase the values of λ1 and 2 to make the generated music sounds closer to existing music (i.e. those observed in the training set).
As the major task considered in this paper is melody generation, for training MidiNet we need a MIDI dataset that clearly specifies per file which channel corresponds to the melody. To this end, we crawled a collection of 1,022 MIDI tabs of pop music from TheoryTab, 4 which provides exactly two channels per tab, one for melody and the other for the underlying chord progression. With this dataset, we can implement at least two versions of MidiNets: one that learns from only the melody channel for fair comparison with MelodyRNN [33], which does not use chords, and the other that additionally uses chords to condition melody generation, to test the capacity of MidiNet.
Network Specification
Our model was implemented in TensorFlow. For the gen- erator, we used as input random vectors of white Gaussian noise of length l = 100. Each random vector go through two fully-connected layers, with 1024 and 512 neurons re- spectively, before being reshaped into a 1-by-2 matrix. We then used four transposed convolution layers: the first three use filters of shape 1-by-2 and two strides [8], and the last layer uses filters of shape 128-by-1 and one stride. Accord- ingly, our conditioner has four convolution layers, which use 128-by-1 filters for the first layer, and 1-by-2 filters for the other three. For creating a monophonic note sequence, we added a layer to the end of G to turn off per time step all but the note with the highest activation.

To evaluate the aesthetic quality of the generation result, a user study that involves human listeners is needed. We conducted a study with 21 participants. Ten of them un- derstand basic music theory and have the experience of be- ing an amateur musician, so we considered them as people with musical backgrounds, or professionals for short. We compared MidiNet with three MelodyRNN mod-
els pre-trained and released by Google Magenta: the basic RNN, the lookback RNN, and the attention RNN [33]. We randomly picked 100 priming melodies from the training data 7 and asked the models create melodies of eight bars by following these primers. We considered two variants of MidiNet in the user study: model 1 (Section 4.2.1) for fair comparison with MelodyRNN, and model 2 (Section 4.2.2) for probing the effects of using chords. Although the result of model 2 was generated by additionally following the chords, we did not playback the chord channel in the user study. We randomly selected the generation result of three out
of the 100 priming melodies for each participant to listen to, leading to three sets of music. To avoid bias, we ran- domly shuffled the generation result by the five considered models, such that in each set the ordering of the five mod- els is different. The participants were asked to stay in a quiet room separately and used a headphone for music lis- tening through the Internet, one set at a time. We told them that some of the music “might be” real, and some might be generated by machine, although all of them were actu- ally automatically generated. They were asked to rate the generated melodies in terms of the following three metrics: how pleasing, how real, and how interesting, from 1 (low) to 5 (high) in a five-point Likert scale.


This work was inspired by the legendary article by Andrej Karpathy - The Unreasonable Effectiveness of Recurrent Neural Networks \cite{AndrejKarpathy2015}. In it Karpathy shows how a neural network consisting of 2 layers of 512 LSTM cells can generalize over several datasets of textual data: Paul Graham essay, Shakespeare plays, XML and Linux C/C++ source code. The resulting model can generate new sequences that are reasonably close to the source material and can almost fool a human, and in the case of C/C++ code some of the results compile. It is a character-level language model, i.e., it learns the texts letter by letter. Our first experiment was to adapt this model to the task of music generation.


\section{Пристап}
\section{Цел}
\subsection{Импровизации} 
\subsection{Помогната копозиција} 
\subsection{Самостојо генерирање} 

\chapter{Машинско учење}

\section{Длабоко учење}

\section{Невонски мрежи}

\cite{Sturm2016} A deep neural network is one that has more than one hidden layer of units (neurons) between its input and output layers [25]. Essentially, a neural network transforms an input by a series of cascaded non-linear operations. A recurrent neural network (RNN) is any neural network possessing a directed connection from the output of at least one unit into the input of another unit located at a shallower layer than itself (closer to the input). A deep RNN is a stack of several RNN layers, where each hidden layer generates an output sequence that is then used as a sequential input for the deeper layer. With deeper architectures, one expects each layer of the network to be able to learn higher level representations of the input data and its short- and long-term relationships.
The recurrence (feedback) present in an RNN allows it to take into account
its past inputs together with new inputs. Essentially, an RNN predicts a sequence of symbols given an input sequence. Training it entails modifying the parame- ters of its transformations to diminish its prediction error for a dataset of known sequences. The basic recurrent structure, however, presents problems related to exploding and vanishing gradients during the training procedure [20, 30], which can result in a lack of convergence of solutions. These problems can be circum- vented by defining the hidden layer activation function in a smart way. One such approach defines long short term memory (LSTM) “cells”, which increases the number of parameters to be estimated in training, but controls the flow of information in and out of each cell to greatly help with convergence [16, 21]. Though RNN and LSTM are not new, recent advances in efficient training
algorithms and the prevalence of data have led to great success when they are applied to sequential data processing in many domains, e.g., continuous hand- writing [15], speech recognition [16], and machine translation [35]. In the next subsection, we describe past applications of recurrent networks to music tran- scription modelling and generation.
  
\section{Рекурентни невронски мрежи}

\section{ЛСТМ}
 
\section{Конволуциски мрежи}
 
\section{Енкодер-Декодер / Авто енкодер}

\section{Учење на секвенци}

\section{Embedding: word and latent space}

\section{Gibbs sampling / Гибсово семплирање}

\cite{Hadjeres2016} ALGORITHM Generation in dependency networks is performed using the pseudo-Gibbs sampling procedure. This Markov Chain Monte Carlo (MCMC) algorithm is described in Alg.1. It is similar to the classical Gibbs sampling procedure (Geman & Geman, 1984) on the difference that the conditional dis- tributions are potentially incompatible (Chen & Ip, 2015). This means that the conditional distributions of Eq. (2) do not necessarily comes from a joint distribution p(V) and that the theoretical guarantees that the MCMC converges to this stationary joint distribution vanish. We experimen- tally verified that it was indeed the case by checking that the Markov Chain of Alg.1 violatesKolmogorov’s criterion (Kelly, 2011): it is thus not reversible and cannot converge to a joint distribution whose conditional distributions match the ones used for sampling. However, this Markov chain converges to another station- ary distribution and applications on real data demonstrated that this method yielded accurate joint probabilities, espe- cially when the inconsistent probability distributions are learned from data (Heckerman et al., 2000). Furthermore, nonreversible MCMC algorithms can in particular cases be better at sampling that reversible Markov Chains (Vucelja, 2014). 2.3.2. We emphasize on this section the importance of our partic- ular choice of data representation with respect to our sam- pling procedure. The fact that we obtain great results using pseudo-Gibbs sampling relies exclusively on our choice to integrate the hold symbol into the list of notes. Indeed, Gibbs sampling fails to sample the true joint dis-
tribution p(V|M, θ) when variables are highly correlated, creating isolated regions of high probability states in which theMCMCchain can be trapped. However, many data rep- resentations used in music modeling such as
• the piano-roll representation,
• the couple (pitch, articulation) representation where articulation is a Boolean value indicating whether or not the note is played or held,
tend to make the musical data suffer from this drawback.
As an example, in the piano-roll representation, a long note is represented as the repetition of the same value over many variables. In order to only change its pitch, one needs to change simultaneously a large number of variables (which is exponentially rare) while this is achievable with only one variable change with our representation.

\chapter{}
Monophonic music composition is the art of creating a single melodic line with no accompaniment. To compose a melody a human composer uses his/her creativity and musical knowledge. In our model composer function C generates a melodic line based on knowledge represented by cumulative frequency distribution matrix CFM.

\chapter{Евалуација}

Which is better?

An interesting experiment that was performed in 2016 by the Sony Computer Science Laboratories in Paris tested people's ability to identify AI generated music: 1,600 people were asked to listen to two distinct harmonies of the same melody – one by Bach and one by an AI called “DeepBach’”. The results showed that more than half of the listeners attributed the DeepBach-generated harmonies to Bach himself, while 75percent  of listeners were able to identify Bach’s music. It’s worth mentioning that a fourth of the testers were professional musicians or music students.
While it’s becoming harder and harder to distinguish between human and AI generated music, the question of which of them is better will probably remain open forever. After all, when it comes to music, it all comes down topersonal taste. One advantage that AI does have is the ability to analyze people's personal taste in order to adjust songs so they are more suited to their tastes and by this reduce the risk of failure. Would this ability prevail over the human "creative spark”? only time will tell.

\cite{Hadjeres2016} Discrimination Test: “Bach or Computer” experiment
Subjects were presented series of only one musical extract together with the binary choice “Bach” or “Computer”. Fig. 5 shows howthe votes are distributed depending on the level of musical expertise of the subjects for each model. For this experiment, 1272 people took this test, 261 with musical expertise 1, 646 with musical expertise 2 and 365 with musical expertise 3.
Subjects were asked to give information about their musical expertise. They could choose what category fits them best between:
1. I seldom listen to classical music
2. Music lover or musician
3. Student in music composition or professional musi- cian.
Interactive composition 4.1. Description
We developed a plugin on top of the MuseScore music editor allowing a user to call DeepBach on any rectangu- lar region. Even if the interface is minimal (see Fig.7), the possibilities are numerous: we can generate a chorale from scratch, reharmonize a melody and regenerate a given chord, bar or part. We believe that this interplay between a user and the system can boost creativity and can interest a wide range of audience.

\cite{Liang2017} 
To measure BachBot’s success in this task, we devel- oped a publicly accessible musical discrimination test at bachbot.com. Unlike prior studies which leverage paid services like Amazon MTurk for human feedback [35], we offered no such incentive and promoted the study only through social media. Participants were first surveyed for their age group and prior music experience (fig. 3a). Next, they are presented five discrimination tasks which presented two audio tracks (an original Bach composition and a synthetic composition by BachBot) and ask them to identify the Bach original. Each audio track contains an entire composition from start to end. The music score for the audio was not provided. Participants were granted an unlimited amount of time and allowed to replay each track an arbitrary number of times. Participants could only see the next question after submit- ting the current one and were not allowed to modify their responses after submitting. The five questions comprised of three harmonizations (S/A/T/B, one AT, one ATB), and two original compositions. To construct the questions, harmonizations were paired along with the original Bach chorales the fixed parts were taken from. No such direct comparison is possible for the SATB case, so these synthetic compositions were paired with a randomly selected Bach chorale in a some- what different comparative listening task. Harmonizations were synthesized by extracting part(s) from a randomly se- lected Bach chorale and filling in the remaining parts of the composition using the method previously described in sec- tion 2.4. Original compositions (questions labelled SATB) were generated by providing a START symbol followed by ancestral sampling as previously described in section 2.3 until an END symbol is reached. The final audio provided in the questions were obtained by rendering the composi- tions using the Piano instrument from the Fluid R3 GM SoundFont.
We only considered the first response per IP address of participants who had played both choices in every question at least once and completed all five questions. This totaled 2336 participants at the time of writing, making our study one of the largest subjective listening evaluation of an au- tomatic composition system to date.
An informal analysis sug- gests that while some neurons are ambiguous to interpreta- tion, other neurons correlate significantly with recognized music-theoretic objects, particularly chords (see fig. 4). To our knowledge, this is the first reported evidence for an LSTM optimized for automatic composition learning music-theoretic concepts without explicit prior informa- tion. This invites a follow-up study testing the statistical significance of these observations.

\cite{MidiNet} To evaluate the aesthetic quality of the generation result, a user study that involves human listeners is needed. We conducted a study with 21 participants. Ten of them un- derstand basic music theory and have the experience of be- ing an amateur musician, so we considered them as people with musical backgrounds, or professionals for short. We compared MidiNet with three MelodyRNN models pre-trained and released by Google Magenta: the basic RNN, the lookback RNN, and the attention RNN [33]. We randomly picked 100 priming melodies from the training data 7 and asked the models create melodies of eight bars by following these primers. We considered two variants of MidiNet in the user study: model 1 (Section 4.2.1) for fair comparison with MelodyRNN, and model 2 (Section 4.2.2) for probing the effects of using chords. Although the result of model 2 was generated by additionally following the chords, we did not playback the chord channel in the user study. We randomly selected the generation result of three out of the 100 priming melodies for each participant to listen to, leading to three sets of music. To avoid bias, we ran- domly shuffled the generation result by the five considered models, such that in each set the ordering of the five mod- els is different. The participants were asked to stay in a quiet room separately and used a headphone for music lis- tening through the Internet, one set at a time. We told them that some of the music “might be” real, and some might be generated by machine, although all of them were actu- ally automatically generated. They were asked to rate the generated melodies in terms of the following three metrics: how pleasing, how real, and how interesting, from 1 (low) to 5 (high) in a five-point Likert scale.

\section{Rangiranje}

\cite{GarciaSalas2011}

 \cite{GarciaSalas2011} Во трудот е опишана и евалуација на резултатите во стил на Турингов тест, на 30 учесници од различна возраст и познавање на музика, но поголем дел од ИТ индустрија, им биле пуштени 10 песни, 5 генеирани од човек, а 5 од алгоритмот, кои ги рангирале по тоа колку им се допаѓаат. Две од генерираните песни биле рангирани на позиција 3 и 4. 


\chapter{Податоци}

\section{Податочно множество}

\section{Податочна репрезентација }

Изворно податочно множество, влез на системот, излез од системот. 

\cite{Tikhonov2017} For each note we were building a note embedding that corresponded to the pitch of the note, an octave embedding that corresponded to the octave of the note and a delay embedding that corresponded to the length of the note. We were using this three embeddings and meta-information of a given MIDI track to build a concatenated note representation that was used as an input for training throughout this paper.

\subsection{Репрезентација на времето}
4.4.1 Global vs Time Slice
The representation of time is fundamental for musical processes. There is a first decision about the temporal scope of the representation (and its relation to the temporal nature of the architecture used):
• global – In this first case, there is no notion of temporal sequence and no explicit notion of time. The granularity of processing by the deep network architecture is the represen-
tation as a whole. The architecture used is not recurrent (typically a feedforward archi- tecture or an autoencoder). Examples are the MiniBach system (see Section 7.1.1.1) and the DeepHear system (see Section 7.1.3.1).
• time step (or time slice) – In this second case, the most frequent one, the atomic temporal granularity of the input (training input or generation input) is a local temporal slice
of the musical content, corresponding to a specific temporal moment (time step). The granularity of processing by the deep network architecture is a time step. Note that the time slice is usually set to the shortest note duration (see Section 4.4.3), but it may be set larger, e.g., to a measure in the system discussed in [106].
• note step – This third case, rare, is proposed byWalder in [113]. In this approach, there is no fixed time step. The granularity of processing by the deep network architecture is
a note. See [113] for more details. A corollary of this design decision is that in the global case, the representation of a
musical data used as a a training input and as a generation input needs to have a fixed size (the number of time steps), whereas in the time step and note step cases, the sequence size is variable: actual lengths of the training input, the generation input and the generated output may be different.

\subsection{Краеви на нотите}
4.4.2 Note Ending
Another important issue is about the note ending, i.e. how is (or is not) represented the end of a note. In the MIDI representation format, the end of a note is explicitly stated (via a Note Off event13). In the piano roll14 notation shown in Section 4.3.2, there is no explicit representation of the ending of a note and, as a result, one cannot distinguish between two
repeating quarter notes ˇ “ ˇ “ and a half note ˘ “. In [21], Eck and Schmidhuber mention two strategies to address this limitation:
• a first strategy (and the most common one) is to divide by 2 the size of the time step (time slice)15 and always mark note endings with a special tag, e.g., 0. The advantage is
that one does not need to change input and target data structures;
• an alternative strategy is to have a special computing unit(s) in the network to indicate the beginning of a note. This method is employed by Todd in [106].

\subsection{Резолуција}
4.4.3 Time Quantization
Some global time quantization, i.e. the definition of the value of the time step is needed to temporally interpret the representation. Eck and Schmidhuber [21] mentions two alternative strategies:
• most commonly, the time step is set to the smallest duration of a note in the corpus
(training examples/dataset), e.g., a sixteenth note ˇ “) . One immediate consequence of this “leveling down” is the number of processing steps necessary independently of the duration of actual notes;
• an alternative strategy, interesting to be noted, was devised by Mozer in the CONCERT system [77] (see Section 7.3.1.1), who used a distributed encoding of duration that al-
lowed him to process a note of any duration in a single network processing time step. By representing in a single time step a note rather than a slice of time, the number of time steps to be bridged by the network in learning global structure is greatly reduced. The approach ofWalder for a note granularity of processing (see Section 4.4.1) is analog. In this strategy, there is no uniform discretization of time (time slice) and no need for.

4.4.3 Time Quantization
Some global time quantization, i.e. the definition of the value of the time step is needed to temporally interpret the representation. Eck and Schmidhuber [21] mentions two alternative strategies:
• most commonly, the time step is set to the smallest duration of a note in the corpus
(training examples/dataset), e.g., a sixteenth note ˇ “) . One immediate consequence of this “leveling down” is the number of processing steps necessary independently of the duration of actual notes;
• an alternative strategy, interesting to be noted, was devised by Mozer in the CONCERT system [77] (see Section 7.3.1.1), who used a distributed encoding of duration that al-
lowed him to process a note of any duration in a single network processing time step. By representing in a single time step a note rather than a slice of time, the number of time steps to be bridged by the network in learning global structure is greatly reduced. The approach ofWalder for a note granularity of processing (see Section 4.4.1) is analog. In this strategy, there is no uniform discretization of time (time slice) and no need for.

\subsection{Аудио сигнал}

\subsection{MIDI}

MIDI (Musical Instrument Digital Interface) is a technical standard that describes a proto- col, a digital interface and connectors for interoperability between various electronic musi- cal instruments, software and devices [73]. MIDI carries event messages that specify note information (such as pitch and velocity) as well as control signals for parameters (such as volume, vibrato and clock signals). There are five types of messages and here we only consider the Channel Voice type, which transmits real-time performance data over a single channel. Two important (for our concerns) messages are:
• Note on – To indicate that a note is (or has to be) played. It contains a status information (what channel number is concerned, specified by an integer within [0 15] and two
data values: a MIDI note number (the note’s pitch, an integer within [0 127]) and a velocity (that indicates how loud the note is played3, an integer within [0 127]). An example is <Note on, 0, 60, 50> which interprets as: “on channel 1, start playing a middle C with velocity 50”.
• Note off – To indicate that a note ends. In that situation, velocity indicates how fast the note is released. An example is <Note off, 0, 60, 20> which interprets as: “on
channel 1, stop playing a middle C with velocity 20”.
Each note event is actually embedded into a track chunk, a data structure containing a delta-time value which specifies the timing information and the event itself. A delta-time value represents the time position, as an absolute value, of the event and could represent:
• a metrical time – It represents the number of ticks from the beginning. A reference, named the division and defined in the file header, specifies how many ticks per quarter
note ˇ “, or a time-code-based time – Not detailed here. An example of extract from a MIDI file (turnt into readable ascii) and its corresponding
score are shown at Figures 4.2 and 4.3. The division has been set to 384, i.e. 384 ticks per quarter note ˇ “, which corresponds to 96 ticks for an eighteenth note ˇ “) .
In Huang and Hu claim that one drawback of encoding MIDI messages directly
is that it does not effectively preserve the notion of multiple notes being played at once through the use of multiple tracks. Since they concatenate tracks end-to-end, they posit that it will be difficult for such a model to learn that multiple notes in the same position across different tracks can really be played at the same time.

\subsection{Текст}

ABC \cite{Sturm2015}
ABC notation is a shorthand form of musical notation. In basic form it uses the letters A through G to represent the given notes, with other elements used to place added value on these - sharp, flat, the length of the note, key, ornamentation. Later, with computers becoming a major means of communication, others saw the possibilities of using this form of notation as an ASCII code that could facilitate the sharing of music online, also adding a new and simple language for software developers. In this later form it remains a language for notating music using the ASCII character set. The earlier ABC notation was built on, standardized and changed to better fit the keyboard and an ASCII character set by Chris Walshaw, with the help and input of others. Although now re-packaged in this form, the original ease of writing and reading, for memory jogs and for sharing tunes with others on a scrap of paper or a beer coaster remains, a simple and accessible form of music notation, not unlike others, such as tablature and solfège. Originally designed for use with folk and traditional tunes of Western European origin, e.g., English, Irish, Scottish, which are typically single-voice melodies that can be written on a single staff in standard notation, the work of Chris Walshaw and others has opened this up with an increased list of characters and headers in a syntax that can also support metadata for each tune.[1]

ABC notation being ASCII-based, any text editor can be used to edit the music. Even so, there are now many ABC notation software packages available that offer a wide variety of features, including the ability to read and process ABC notation into MIDI files and as standard "dotted" notation. Such software is readily available for most computer systems, including Microsoft Windows, Unix/Linux, Macintosh, Palm OS, and web-based.[2]

Later third-party software packages have provided direct output, bypassing the TeX typesetter,[3] and have extended the syntax to support lyrics aligned with notes,[4] multi-voice and multi-staff notation,[5] tablature,[6] and MIDI.[7]


Chord tabs

Chrods cite chord2vec

A melody can be encoded in a textual representation and processed as a text. A significant example is the ABC notation [115], a de facto standard for folk and traditional music4. See at Figures 4.6 and 4.7 the original score and its associated ABC notation of a music named “A Cup of Tea”, from the repository and discussion platform The Session [56]. The first 6 lines are the header and represent metadata (e.g., T: title of the music, M:
meter (it is actually the time signature), L: default length, K: key. . . ). It is followed by the main text representing the melody. We illustrate below some basic principles of the encoding rules (please refer to [115] for the details):
• the pitch class5 of a note is encoded as the letter corresponding to its english notation (e.g., A for A or La);
• its pitch is encoded as following: A corresponds to A46, a to an A one octave up and a’ to an A two octaves up;
• the duration of a note is encoded as following: if default length is marked as 1/8 (i.e. an eighth note7
ˇ “( – the case for this example), a corresponds to an eighth note ˇ “( , a/2 to a sixteenth note ˇ “) and a2 to a quarter note ˇ “;
• measures (also named bars)8 are separated by | (bars). Note that the ABC notation can only represent monodic melodies. This representation
is for instance used by Sturm in [97] (see Section 7.3.1.2). 4.3.4
A representation of a chord could be extensional, enumerating the notes composing it, or intensional, specifying the pitch class of its root note (e.g., C, A. . . ) and its type (e.g., major, minor, dominant seventh. . . ), usually using an abbreviated jazz notation, e.g., C, D-, E7. . . 9. In most cases, the abbreviated notation is chosen, as in Jazz and popular music. In summary, a chord is usually represented by a pair < pitchclass,type >, where pitch
class ∈ {A, A?10, B, . . . , G, G?} and the set of possible types is predefined (e.g., {+, -, 7, -7, +, -7(?5), 11, ?13(?9). . .}). Note that the way, standard in Jazz, to indicate a note other than the root (of the chord) as to be played by the bass (e.g., A-7/E11) is an additional issue most of time not considered in music generation.

An interesting alternative representation of chords, named Chord2Vec (inspired by the
Word2Vec model for natural language [70]), has been recently proposed in [68]12. Rather than thinking of chords (vertically) as vectors, they represent chords (horizontally) as se- quences of constituent notes. More precisely: 1) a chord is represented as an (arbitrary length) ordered sequence of notes and 2) chords are separated by a special symbol (as for sentence markers in natural language processing). When using this representation for predicting neighboring chords, a specific compound architecture is used, named RNN Encoder-Decoder, offering a very accurate model (see Section 7.3.4.1). Note that a somehow similar model has been used for polyphonic music generation in
the BachBot system [66] (analyzed in Section 7.3.1.3). In this model, for each time step, the various notes are represented as a sequence and a special delimiter symbol (|||) indicates
the next time frame (with constant time step of an eighth note ˇ “( ). Notes are ordered, in a descending pitch (soprano voice being the first one). Each note is encoded as its MIDI pitch
value and a boolean indicating if it is tied to a note at the same pitch from previous frame. An example is shown at Figure 4.8, encoding two successive chords (the first having the duration of a quarter note) and the second one possessing a fermata (noted as (.)).

\subsection{Пиално лента}

The piano roll representation of a melody (monodic or polyphonic) is inspired from au- tomated pianos (see Figure 4.4). This was a continuous roll of paper with perforations (holes) punched into it. Each perforation represents a note control information, to trigger a given note. The length of the perforation corresponds to the duration of a note. On the other dimension, the localization of a perforation corresponds to its pitch. An example of a modern piano roll representation (for digital music systems) is shown
at Figure 4.5. The x axis represents time and the y axis the pitch. In that example, two voices are encoded. The piano roll is one of the most frequent representations used, although it has some limitations. An important one, comparing to MIDI representation, is that the information of the note off does not exist. As a result, there is no way to distinguish between a long note and two short notes (see Section 4.4.2). The experiment conducted by Huang and Hu [47] is interesting in that they compare using MIDI and piano roll formats. See also the publication byWalder entitled “Modelling Symbolic Music: Beyond the Piano Roll” [113]

\subsection{Distributed note representation / circle of fifths}

Mozer [29] builds RNN to model and generate melody using a distributed
approach to music encoding. These systems generate output at the note level rather than at uniform time steps. Each pitch is encoded based on its fundamen- tal frequency, chromatic class, and position in the circle of fifths. Note duration is encoded using a similar approach. Chordal accompaniment is encoded based on the pitches present. Some input units denote time signature, key, and down-
beats. Mozer’s

\cite{Biles1994} They only allow for 14 pitches in the melody, but those are релатив, their absolute values are chose according to a mapped chrod. A chord map is created for each half measure. Each map is an array of 14 MIDI pitches. A chord is mapped to a scale strictly vertically. After a scale is selected, it is extended to 14 tones, beginning at or above middle C. Interesting representation.

\section{Предпроцесирање, трансформации и филтрирање}

\cite{Sturm2016} We create data for training our folk-rnn model in the following way. We
remove title fields and ornaments. We remove all transcriptions that have fewer than 7 measures when considering repetitions (to remove contributions that are not complete transcriptions, but transcriptions of suggested endings, variations, etc.). We remove all transcriptions that have more than one meter or key.11 We transpose all remaining transcriptions (23,636) to a key with root C.
We impose a transcription token vocabulary — each token consists of one or more characters — for the following seven types (with examples in parens): meter (“M:3/4”), key (“K:Cmaj”), measure (“:|” and “|1”), pitch (“C” and “c’”),
grouping (“(3”), duration (“2” and “/2”), and transcription (“<s>” and “<\ s>”). 
Our dataset has 4,056,459 tokens, of which 2,816,498 are pitch, 602,673 are du- ration, and 520,290 are measure. A majority of the 23,636 transcriptions consists of 150 tokens or fewer; and 75percent have no more than 190. There are 137 unique tokens, each of which becomes a vocabulary element for our folk-rnn model.

\cite{Yang2017} 
For simplicity, we filtered out MIDI tabs that contain chords other than the 24 basic chord triads (12 major and 12 minor chords). Next, we segmented the remaining tabs every 8 bars, and then pre-processed the melody channel and the chord channel separately, as described below. For melodies, we fixed the smallest note unit to be the sixteenth note, makingw = 16. Specifically, we prolonged notes which have a pause note after them. If the first note of a bar is a pause, we extended the second note to have it played while the bar begins. There are other exceptions such as triplets and shorter notes (e.g. 32nd notes), but we chose to exclude them in this implementation. More- over, for simplicity, we shifted all the melodies into two oc- taves, from C4 to B5, and neglected the velocity of the note events. Although our melodies would use only 24 possible notes after these preprocessing steps, we considered all the 128 MIDI notes (i.e. from C0 to G10) in our symbolic representation. In doing so, we can detect model collaps- ing [12] more easily, by checking whether the model gen- erates notes outside these octaves. As there are no pauses in our data after preprocessing, we do not need a dimension for silence. Therefore, h = 128. For chords, instead of using a 24-dimensional one-hot vector, we found it more efficient to use a chord representa- tion that has only 13 dimensions— the first 12 dimensions for marking the key, and the last for the chord type (i.e. major or minor), as illustrated in Table 2. We pruned the chords such that there is only one chord per bar. After these preprocessing steps, we were left with 526 MIDI tabs (i.e. 4,208 bars). 5 For data augmentation, we circularly shifted the melodies and chords to any of the 12 keys in equal temperament, leading to a final dataset of 50,496 bars of melody and chord pairs for training.

\cite{Tikhonov2017}  1. Spliting the midi tracks and filtering only the desired ones by heuristic. 2. Removed strenght/velocity of the note being played, to focus on the melodic pattern determined by the pitches and temporal parameters of the notes and pauses in between and not performance nuances. 3. In order to make the learning state space denser we have centered the pitches throughout the dataset transposing median pitch of every track to the 4th octave. 4. We also normalized the pauses throughout the dataset in the following manner. For each track we have calculated a median pause. It is only to be expected that absolute majority of the pauses in the track were equal to the median pause multiplied with a rational coefficient (naturally 1/2 and 3/2 were especially popular in the majority of the tracks). We counted all possible pauses in every track and left only the tracks that had 11 different values of the pauses or less (the median + most popular pauses on each side of it). The tracks with higher variety of pauses were not included in the final dataset.
5. Finally to make the input diverse enough we have filtered the tracks with exceedingly small entropy.


\subsection{Транспозиција}

A common technique in machine learning is to generate synthetic data as a way to artifi- cially augment the size of the dataset (the number of training examples) in order to improve the learning. In the musical domain, a natural and easy way is transposition, i.e. to trans- pose all examples in all keys21. In addition to artificially augment the dataset, this provides a key (tonality) invariance of all examples and thus makes the examples more generic. This also reduces sparsity in the training data. This transposition technique is for instance used in [61], described in Section 7.7.1.1. An opposed approach is to transpose (align) all ex- amples into a single common key. This has been advocated by [7] to facilitate learning (see Section 7.3.2.1).
4.4.9

\cite{Bretan2016} 3.1 Design of a Musical DBN Autoencoder
In order to analyze and reconstruct a melody we trained a deep autoencoder to encode and decode a single measure of music. This means that our unit (in this scenario) is one measure of music. From the dataset there are roughly 170,000 unique measures. Of these, there are roughly 20,000 unique rhythms seen in the measures. We augment the dataset by manipulating pitches through linear shifts (transpositions) and alterations of the intervals between notes resulting in roughly 80 million unique measures. We augment the dataset by manipulating pitches through linear shifts (transpositions) and alterations
of the intervals between notes. We alter the intervals using two methods: 1) adding a constant value to the original intervals and 2) multiplying a constant value to the intervals. Many different constant values are used and the resulting pitches from the new interval values are superimposed on to the measure’s original rhythms. The new unit is added to the dataset. We restrict the library to measures with pitches that fall into a five octave range (midi notes 36-92). Each measure is transposed up and down a half step so that all instances within the pitch range are covered. The only manipulation performed on the duration values of notes within a measure is the temporal compression of two consecutive measures into a single measure. This “double time" representation effectively increases the number of measures, while leaving the inherent rhythmic structure in tact. After all of this manipulation and augmentation there are roughly 80 million unique measures. We use 60% for training and 40% for testing our autoencoder. The first step in the process is feature extraction and creating a vector representation of the unit. Unit
selection allows for a lossy representation of the events within a measure. As long as it is possible to rank the units it is not necessary to be able to recreate the exact sequence of notes with the autoencoder. Therefore, we can represent each measure using a bag-of-words (BOW) like feature vector. Our features include:
1. counts of note tuples <pitch1, duration1> 2. counts of pitches <pitch1> 3. counts of durations <duration1> 4. counts of pitch class <class1> 5. counts of class and rhythm tuples <class1, duration1> 6. counts of pitch bigrams <pitch1, pitch2>

\subsection{Филтрирање}



\chapter{Архитектури}

\section{Едноставен ЛСТМ модел}

\section{Проширен ЛСТМ модел}

\section{Енкодер-Декодер модел}

\chapter{Tools}

\section{Keras}

\section{Music21}

\section{pypianoroll}

\chapter{Студија на случај}

\section{Податочно множество}

Најдостапен и најлесен за комјутерска обработка извор на симболична музука претставуваат МИДИ датотеките. Во иницијалните фази на изработка користев мало податочно множество коешто го изградив од GuitarPro датотеки рачно избрани од http://ultimate-guitar.com. Датотеките преставуваат кориснички генерирана репрезентација на музиката, примарно наменети за учење. Бидејќи системот за репродукција на звук за GuitarPro датотеките е базиран на MIDI стандардот, можев лесно да ги конвертирам во MIDI датотеки. Финалниот чекор за подготовка на податоците е трансформација во пиано лента репрезентација. За жал процесот резултираше во неквалитетна конверзија, песните во финалната форма има многу аномалии, неправилности и изгубена смисла на одредени елементи. Се појавија многу тонови со времетраење од 0 рамки, како и тонови со времетраење од десетици секунди. Исто така се појавија истровремено свирење на 10 и повеќе тонови на гитара, што е невозможно со стандардна гитара. Добар дел од проблемите следат од тоа што датотеките се кориснички генерирани што не гарантира квалитет, како и од алатките за конверзија. Затоа одлучива да ја напуштам оригиналната замисла и да користам готово податочно множество.

\subsection{Лахк (Lahk) MIDI}

Лахк MIDI податочното множество се состои од 176.581 уникатни MIDI датотеки, соберени од страна Рафел за целите на неговоит труд \cite{Raffel2016} во кој покажува систем за пребарување и споредување на секвенци, или исечоци, од MIDI датотеки. Дел од овие множеството, наречен LMD-Matched, се состои од 45.129 датотеки спарени со соодвенти метаподатоци од датабазата MillionSongs, што овозможува пребарување, групирање и селекција по одредени критериуми како жанр, автор, стил, темпо и сл. 
Врз основа на LMD-Matched податочното множество, Донг за потребите на трудот \cite{Dong2017} го имат конвертирано целото податочно множество во пиано лентал. На веб страната на која е прикажан нивниот труд се достпни повеќе верзии од множеството:

\begin{itemize}
    \item LPD-Cleansed - Претставува прочистена верзија на податочното множество според следните критериуми: отстранети песни кои не се со 4/4 ритам, отстранети песни со повеќе од една промена на ритам, остранети песни каде што првиот такт не почнува од нулти момент, задржани песните кои што со наголема сигурност се спарени со ставка од MillionSongs датабазата.
    \item LPD-5 - Сите можни инстанци на инструменти во MIDI датотеките се групирани во следните 5 категории: тапани, пиано, гитара, бас и жичана секција; според MIDI програмскиот број на инструментот. Неголем дел од инструментите кои не припаѓаат на ниту една од овие категории се групирани во жичаната секција, со исклучок на синтисајзер, перкусии и звучни ефекти.
    \item LPD-17 - Слично на претходното множество со тоа што инструментите се групирани во 17 категории: тапани, пиано, хроматски перкусии, оргуља, гитара, бас, жичана секција, ансамбл, метални дувачки инструменти, дрвени дувачки, писка, главен синтисајзер, придружба синтисајзер, етнички инструменти, перкусии и звучни ефекти.
\end{itemize}

LPD-5 податочното множество се покажа како одлична појдовна точка за изградба на помало податочно множество за експериментите, бидејќи содржи песни со константен 4/4 ритам. Меѓутоа не е доволно добро за употреба без модификација. Не сите песни ги имаат сите инструменти, а некои песни имаат повеќе инструменти групирани во една трака, така да потребна е додатна обработка и селекција за да се добие квалитетно податочно множество.

\section{Податочна репрезентација}

Песните во LPD-5 податочното множество се во пиано лента репрезентација, каде што времето е поделено така што една рамка од лентата претставува 1/24 од еден такт. Една рамка се содржи од 128 целобројни вредности, по една за секоја од можните ноти или звуци опишани со MIDI стандардот, при што вредноста го опипува интензитетот со кој се присутен звукот. Ваквиот вектор не е адекватен за учење на секвенци, бидејќи преставува решавање на 128 регресионони проблеми во паралела. Затоа првиот чекор во прилагодување на векторот претставува бинаризација, т.е. во секоја рамка секоја ненулта вредност се заменува со единица. Во обратна насока, при дебинаризација на векторот, единиците ги заменувам со рачно одредена вредност за интензитет, 2/3 од максималната вреднос. На овој начин се губи експресивност и нуанса, меѓутоа драстично се олеснува проблемот на учење на секвенци. 
По иницијални експерименти каде што тренирав модел да учи секвенци од тн. "multi-hot" вектори (бинарни вектори каде што повеќе вредности може да бидат активни истовремено), како што е бинаризираната варијанта на пиано ленти, дојдов до заклучок дека ваквите секвенци претставуваат тежок проблем за учење за моделите коишто сакав да ги искористам. Учење на 128 паралелни проблеми резултираше во многу ретки активации на излезниот слој, а во рамките кадешто се појавија повеќе активации, комбинациите на ноти често беа бесислени. Поради тоа одлучив да изградам лексикон од сите можни комбинации на ноти, и да ги користам елементите од лексиконот како влезе во моделот за учење. Трансформацијата ја извршив така што 128 елементниот вектор го сметам како 128 битна бинарна репрезентација на цел број. Лексиконот го изградив од ваквите целобројни репрезентации со додатни симобили за почеток и крај на песна/секвенца. Лексиконот потоа го индексирав и секој елемент доаѓа на влез на моделот во "one-hot" или 1-n бинарен вектор, во кој што сите вредности се нула, само вредноста со ист индекс како елементот од лексиконот е единица.

\section{Селекција на песни}

Датотеките од податочното множество содржат 5 траки кои ги преставуваат инструментите: тапани, пиано, гитара, бас и жичана секција. Меѓутоа не сите датотеки имаат податоци во сите траки, т.е. не е извршена претходно филтрирање на песните така што сите песни да ги имаат сите инструменти. Додатно бидејќи во жичана секција се групирана многу инструменти, што често може да доведе и до групирање на повеќе траки од изворната датотека во една, одлучив да ги филтрирам сите датотеки што содржат податоци на таа трака. Додатно голем број на песни не содржат пиано трака, а бидејки фокусот ми беше на рок песни коишто најчесто немаат пиано делови, ги исфилтрирав и пените што содржат пиано. Останатите песни ги филтрирав така да пиано лентите за траките тапани, гитара и бас гитара се целосно исполнети. Од добиеното подмножество со случајна селекција избрав 500 песни, од коишто поголем дел природно се погодија од рок, поп-рок и метал жанр. Од ова подмножество креирав MIDI датотеки со користење на pypianoroll библиотеката. Потоа со рачна инспекција на песните една по една избрав (ФИНАЛНА БРОЈКА НА ПЕСНИ) според субјективна процена за квалитетот на добиената датотека, познавање на песната, жанровска припадност и релативна едноставност на песната.

\section{Транскрипција на песните}

За збогатување на податочното множество и гарантирање на транслациона инваријанса на мелодиските линии искорив постапка на музичка транскрипција. Прво ги одредив најнискиот и највисокиот тон присутен во избраното податочно множество. Потоа за секоја од песните извришив транскрипција надолу за толку полутонови колку што има од најнискиот тон во податочното множество и најнискиот тон во самата песна, и нагоре за толку полутонови колку што има помеѓу највисокиот тон во податочното множество и највисокиот тон во поесната. Ваквата стратегија резултира во разлилен број на транскрипција по песна, зависно од колку голем е тоналниот опсег на песната, што значи некои песни се поприсутни во резултантното транскрибирано податочно множество, а некои помалку присутни; но и во најголем можен број на транскрипции. Транскрипцијата ја извршив само на траките за гитара и бас, бидејќи нема смисла за тапани, таму миди тоновите не претставуваат ноти на ист начин како кај останатите инструменти, туку тип на удар и инструмент за удар.

(ЦИТИРАЈ ТРУД ЗА ТРАНСКРИПЦИЈА), (БРОЈКА НА ТРАНСКРИБИРАНИ ПЕСНИ)

\section{Упростување на проблемот}

Вака добвиеното потадочно множество претставуше алгоритамски и технички проблеми за процесот на учење. Најлош случај се јавува кај гитарата. Теоретски во најлош случај можни се 128*127*126*125*124*123=3*10e13 комбинации на ноти според MIDI стандардот, или според физичките ограничувања на гитара 24*6=191.102.976 можни комбинации на ноти. Во еден од експериментите со 50 песни со целосна транскрипција не беше возможно извршивање на моделот бидејки лексиконот напросто експлодираше во големина, па не беше возможно извршување на моделот во меморија, на систем со NVIDIA Tesla K80 со 24GB меморија што е горниот лимит на достапен хардвер. Моделот имаше вкуно (ЦИТИРАЈ БРОЈ НА ВЕЛЗНИ ПАРАМЕТРИ) велзни параметри. Покрај тоа што невозможно за изршување, претпоставувам дека би се разредило, т.е. би се разделиле активациите на премногу елементи, и би било потешко за анализа и дебагирање. Затоа одлучив да изврша консолидација на податочното множество и негово упростување.

\subsection{Поедноставување на акордите, намалување на бројот на можни комбинации}

По пребројување на инстанците од сите комобнации на ноти низ податочното множество увидов дека сите категории на акорди не се подеднакво застапени. Најчесто се појавуваат дурски и молски триади, и рок (познати и како "power") акорди како и поединечни ноти. Во (ЛИНК ДО ТАБЕЛА) може да се види честотата на појавување на акордите по категории. Одлучив да ги заменам акордите чии категории не се појавуваат со повеќе од 1\% со основната нота на акордот, а останатите акорди да ги упростам до 3 нотни верзии. Задржани се акордите од следните категории: (ЛИСТА ОД ФОРТЕ КЛАСИ). Оваа трансформација сметам дека нема премногу да го наруши карактерот на музиката, а сепак ќе ја намали пресметковната комплексност во граници на практична изведба.

(ТАБЕЛА НА ПОЈАВУВАЊА НА ТИПОВИ НА АКОРДИ)

\section{Архитектура за учење}

\subsection{Основен ЛСТМ мрежа за секвенци}

\subsection{Ембединг на акорди}

\subsection{Комуникација помеѓу подмрежите}

\section{Алгоритам за генерирање на песни}

\subsection{Паралелно Гибсово семплирање}

\subsection{Температура} 

\section{Анализа на резултати}

\chapter{Толкување и можност за подобрување}

\chapter{Заклучок}