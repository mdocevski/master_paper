%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Eftim at 2015-04-14 11:36:36 +0200 


%% Saved with string encoding Unicode (UTF-8) 

@article{Herremans2017,
abstract = {We present a semantic vector space model for capturing complex polyphonic musical context. A word2vec model based on a skip-gram representation with negative sampling was used to model slices of music from a dataset of Beethoven's piano sonatas. A visualization of the reduced vector space using t-distributed stochastic neighbor embedding shows that the resulting embedded vector space captures tonal relationships, even without any explicit information about the musical contents of the slices. Secondly, an excerpt of the Moonlight Sonata from Beethoven was altered by replacing slices based on context similarity. The resulting music shows that the selected slice based on similar word2vec context also has a relatively short tonal distance from the original slice.},
archivePrefix = {arXiv},
arxivId = {1706.09088},
author = {Herremans, D and Chuan, C},
doi = {10.13140/RG.2.2.22227.99364/1},
eprint = {1706.09088},
journal = {Proceedings of the First International Workshop on Deep Learning for Music},
keywords = {music,music context,neural networks,semantic vector,word2vec},
number = {May},
pages = {11--18},
title = {{Modeling Musical Context Using Word2vec}},
year = {2017}
}

@article{Mehri2016,
abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
archivePrefix = {arXiv},
arxivId = {1612.07837},
author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
eprint = {1612.07837},
pages = {1--11},
title = {{SampleRNN: An Unconditional End-to-End Neural Audio Generation Model}},
year = {2016}
}



@article{Johnson2017,
abstract = {We describe a neural network architecture designed to learn the musical structure of jazz melodies over chord progressions, then to create new melodies over arbi-trary chord progressions from the resulting connectome (representation of neural network structure). Our ar-chitecture consists of two sub-networks, the interval expert and the chord expert, each being LSTM (long short-term memory) recurrent networks. These two sub-networks jointly learn to predict a probability dis-tribution over future notes conditioned on past notes in the melody. We describe a training procedure for the network and an implementation as part of the open-source Impro-Visor (Improvisation Advisor) applica-tion, and demonstrate our method by providing impro-vised melodies based on a variety of training sets.},
author = {Johnson, Daniel D and Keller, Robert M and Weintraut, Nicholas},
journal = {Eighth International Conference on Computational Creativity (ICCC'17)},
pages = {151--158},
title = {{Learning to Create Jazz Melodies Using a Product of Experts}},
year = {2017}
}

@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
isbn = {9781937284961},
issn = {09205691},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
year = {2014}
}

@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
doi = {10.1162/jmlr.2003.3.4-5.951},
eprint = {1310.4546},
isbn = {2150-8097},
issn = {10495258},
pages = {1--9},
pmid = {903},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}

@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
eprint = {1409.3215},
isbn = {1409.3215},
issn = {09205691},
pages = {1--9},
pmid = {2079951},
title = {{Sequence to Sequence Learning with Neural Networks}},
year = {2014}
}

@article{Sturm2016,
abstract = {We apply deep learning methods, specifically long short-term memory (LSTM) networks, to music transcription modelling and composition. We build and train LSTM networks using approximately 23,000 music transcriptions expressed with a high-level vocabulary (ABC notation), and use them to generate new transcriptions. Our practical aim is to create music transcription models useful in particular contexts of music composition. We present results from three perspectives: 1) at the population level, comparing descriptive statistics of the set of training transcriptions and generated transcriptions; 2) at the individual level, examining how a generated transcription reflects the conventions of a music practice in the training transcriptions (Celtic folk); 3) at the application level, using the system for idea generation in music composition. We make our datasets, software and sound examples open and available: $\backslash$url{\{}https://github.com/IraKorshunova/folk-rnn{\}}.},
archivePrefix = {arXiv},
arxivId = {1604.08723},
author = {Sturm, Bob L. and Santos, Jo{\~{a}}o Felipe and Ben-Tal, Oded and Korshunova, Iryna},
eprint = {1604.08723},
journal = {Conference on Computer Simulation of Musical Creativity},
keywords = {algorithmic composition,deep learning,music modelling,recurrent neural network},
pages = {16},
title = {{Music transcription modelling and composition using deep learning}},
year = {2016}
}

@article{Zils2001,
abstract = {This work addresses the issue of retrieving efficiently sound $\backslash$nsamples in large databases, in the context of digital music $\backslash$ncomposition. We propose a sequence generation mechanism called $\backslash$nmusical mosaicing, which enables to generate automatically $\backslash$nsequences of sound samples by specifying only high-level $\backslash$nproperties of the sequence to generate. The properties of the $\backslash$nsequence specified by the user are translated automatically into $\backslash$nconstraints holding on descriptors of the samples. The system we $\backslash$npropose is able to scale up on databases containing more than $\backslash$n100.000 samples, using a local search method based on constraint $\backslash$nsolving. In this paper, we describe the method for retrieving and $\backslash$nsequencing audio samples, and illustrate it with rhythmic and $\backslash$nmelodic musical sequences. },
author = {Zils, Aymeric and Pachet, Fran{\c{c}}ois},
journal = {Digital Audio Effects (DAFx)},
pages = {1--6},
title = {{Musical mosaicing}},
year = {2001}
}

@article{Mozer1994,
author = {Mozer, Michael C.},
journal = {Connection Science},
pages = {247--280},
title = {{Neural network music composition by prediction: exploring the benefits of psychoacoustic constraints and multi-scale processing}},
year = {1994}
}

@article{Yang2017,
abstract = {Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.},
archivePrefix = {arXiv},
arxivId = {1703.10847},
author = {Yang, Li-Chia and Chou, Szu-Yu and Yang, Yi-Hsuan},
eprint = {1703.10847},
title = {{MidiNet: A Convolutional Generative Adversarial Network for Symbolic-domain Music Generation}},
year = {2017}
}



@article{Schwarz2006,
abstract = {The concatenative real-time sound synthesis system CataRT plays grains from a large corpus of segmented and descriptor-analysed sounds according to proximity to a target position in the descriptor space. This can be seen as a content-based extension to granular synthesis providing direct access to specific sound characteristics. CataRT is implemented as a collection of Max/MSP patches using the FTM library and an SQL database. Segmentation and MPEG-7 descriptors are loaded from SDIF files or generated on-the-fly. The object-oriented software architecture follows the model–view–controller design pattern. CataRT allows to explore the corpus interactively or via a target sequencer, to resynthesise an audio file or live input with the source sounds, or to experiment with expressive speech synthesis and gestural control.},
author = {Schwarz, Diemo and Gr{\'{e}}gory, Beller and Bruno, Verbrugghe and Sam, Britton},
journal = {Proc. of the 9th Int. Conference on Digital Audio Effects (DAFx-06)},
number = {September},
pages = {1--7},
title = {{Real-time corpus-based concatenative synthesis with CataRT}},
year = {2006}
}

@misc{Gordon2017,
abstract = {A detailed look into the compositional process, production techniques and creative philosophies behind the hell-raising soundtrack to the 4th installment of the seminal first-person shooter franchise, 'DOOM'. Composer Mick Gordon ('Killer Instinct', 'Wolfenstein: The New Order', 'Need for Speed') will give an insight into how to create a high-energy modern first-person-shooter soundtrack that unashamedly sits front-and-center, appeals to fans and stays true to the franchise. Covering musical sound design, synthesis techniques, compositional approach, interactive music, mixing, working remotely and idea generation, Mick will discuss the fine points behind creating an aggressive soundtrack that both engages the player and supports gameplay. },
author = {Gordon, Mick},
title = {{'DOOM': Behind the Music}},
url = {https://www.gdcvault.com/play/1024068/-DOOM-Behind-the},
year = {2017}
}

@article{GarciaSalas2011,
author = {{Garc{\'{i}}a Salas}, Horacio Alberto and Gelbukh, Alexander and Calvo, Hiram and Soria, Fernando Galindo},
journal = {Polibits},
pages = {59--65},
title = {{Automatic Music Composition with Simple Probabilistic Generative Grammars}},
volume = {44},
year = {2011}
}

@article{Biles1994,
abstract = {This paper describes GenJam, a genetic algorithm-based model of a novice jazz musician learning to improvise. GenJam maintains hierarchically related populations of melodic ideas that are mapped to specific notes through scales suggested by the chord progression being played. As GenJam plays its solos over the accompaniment of a standard rhythm section, a human mentor gives real-time feedback, which is used to derive fitness values for the individual measures and phrases. GenJam then applies various genetic operators to the populations to breed improved generations of ideas.},
author = {Biles, John a.},
isbn = {1026-1087},
issn = {1026-1087},
journal = {Proceedings of the International Computer Music Conference},
pages = {131--137},
title = {{GenJam: A genetic algorithm for generating jazz solos}},
year = {1994}
}

@article{Cope1991,
author = {Cope, David},
journal = {Computer - Special issue: Computer-generated music},
number = {7},
pages = {22--28},
title = {{Recombinant music using the computer to explore musical style}},
volume = {24},
year = {1991}
}

@article{Oord2016,
abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
doi = {10.1109/ICASSP.2009.4960364},
eprint = {1609.03499},
isbn = {9783901882760},
issn = {0899-7667},
pages = {1--15},
pmid = {18785855},
title = {{WaveNet: A Generative Model for Raw Audio}},
year = {2016}
}

@article{Tikhonov2017,
abstract = {A serious problem for automated music generation is to propose the model that could reproduce sophisticated temporal and melodic patterns that would correspond to the style of the training input. We propose a new architecture of an artificial neural network that helps to deal with such tasks. The proposed approach is based on a long short-term memory language model combined with variational recurrent autoencoder. These methods have certain advantages when dealing with temporally rich inputs. The proposed architecture comprises this features and helps to generate results of higher complexity and diversity.},
archivePrefix = {arXiv},
arxivId = {1705.05458},
author = {Tikhonov, Alexey and Yamshchikov, Ivan P.},
eprint = {1705.05458},
keywords = {artificial intelligence,variational recurrent autoencoder},
pages = {1--11},
title = {{Music generation with variational recurrent autoencoder supported by history}},
year = {2017}
}


@article{Boulanger-Lewandowski2012,
abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
archivePrefix = {arXiv},
arxivId = {1206.6392},
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
eprint = {1206.6392},
isbn = {978-1-4503-1285-1},
number = {Cd},
title = {{Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription}},
year = {2012}
}

@article{Boulanger-Lewandowski2014,
abstract = {This thesis focuses on advancing the state of the art in sequence modeling, and thereby improving several applications in the area of polyphonic music and speech, namely polyphonic music generation and transcription, audio chord recognition, speech recognition and audio source separation. Modeling real-world sequences often involves capturing long-term dependencies between the high-dimensional objects that compose such sequences. This problem is in general too dicult to tackle by manually engineering rules to process the data in each possible scenario and we instead follow a machine learning approach.},
author = {Boulanger-Lewandowski, Nicolas},
isbn = {9782951677319},
pages = {145},
title = {{Modeling High-Dimensional Audio Sequences with Recurrent Neural Networks}},
year = {2014}
}


@article{Goel2014,
abstract = {In this paper, we propose a generic technique to model temporal dependencies and sequences using a combination of a recurrent neural network and a Deep Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state of the RNN that allows it to provide temporal information and a multi-layer DBN that helps in high level representation of the data. This makes RNN-DBNs ideal for sequence generation. Further, the use of a DBN in conjunction with the RNN makes this model capable of significantly more complex data representation than an RBM. We apply this technique to the task of polyphonic music generation.},
archivePrefix = {arXiv},
arxivId = {1412.7927},
author = {Goel, Kratarth and Vohra, Raunaq and Sahoo, J. K.},
doi = {10.1007/978-3-319-11179-7_28},
eprint = {1412.7927},
isbn = {9783319111780},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep Belief Networks,Deep architectures,creative machine learning,generative models,music generation,recurrent neural networks},
pages = {217--224},
title = {{Polyphonic music generation by modeling temporal dependencies using a RNN-DBN}},
volume = {8681 LNCS},
year = {2014}
}


@article{Johnson2017,
abstract = {We describe a neural network architecture designed to learn the musical structure of jazz melodies over chord progressions, then to create new melodies over arbi-trary chord progressions from the resulting connectome (representation of neural network structure). Our ar-chitecture consists of two sub-networks, the interval expert and the chord expert, each being LSTM (long short-term memory) recurrent networks. These two sub-networks jointly learn to predict a probability dis-tribution over future notes conditioned on past notes in the melody. We describe a training procedure for the network and an implementation as part of the open-source Impro-Visor (Improvisation Advisor) applica-tion, and demonstrate our method by providing impro-vised melodies based on a variety of training sets.},
author = {Johnson, Daniel D and Keller, Robert M and Weintraut, Nicholas},
journal = {Eighth International Conference on Computational Creativity (ICCC'17)},
pages = {151--158},
title = {{Learning to Create Jazz Melodies Using a Product of Experts}},
year = {2017}
}

@misc{AndrejKarpathy2015,
author = {{Andrej Karpathy}},
title = {{The Unreasonable Effectiveness of Recurrent Neural Networks}},
url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
urldate = {2018-01-23},
year = {2015}
}

@article{Eck2002,
abstract = {In general music composed by recurrent neural networks (RNNs) suffers from a lack of global structure. Though networks can learn note-by-note transition probabilities and even reproduce phrases, attempts at learning an entire musical form and using that knowledge to guide composition have been unsuccessful. The reason for this failure seems to be that RNNs cannot keep track of temporally distant events that indicate global music structure. Long Short-Term Memory (LSTM) has succeeded in similar domains where other RNNs have failed, such as timing {\&} counting and CSL learning. In the current study we show that LSTM is also a good mechanism for learning to compose music. We compare this approach to previous attempts, with particular focus on issues of data representation. We present experimental results showing that LSTM successfully learns a form of blues music and is able to compose novel (and we believe pleasing) melodies in that style. Remarkably, once the network has found the relevant structure it does not drift from it: LSTM is able to play the blues with good timing and proper structure as long as one is willing to listen.},
author = {Eck, Douglas and Schmidhuber, J{\"{u}}rgen},
journal = {Idsia},
pages = {1--11},
title = {{A First Look at Music Composition using LSTM Recurrent Neural Networks}},
year = {2002}
}

@article{Eck2008,
abstract = {This paper addresses the challenge of learning global musical structure from databases of music sequences. We introduce a music-specific sequence learner that combines an LSTM recur-rent neural network with an autocorrelation-based predictor of metrical structure. The model is able to learn arbitrary long-timescale correlations in music but is biased towards finding cor-relations that are aligned with the meter of the piece. This biasing allows the model to work with low learning capacity and thus to avoid overfitting. In a set of simulations we show that the model can learn the global temporal structure of a musical style by simply trying to predict the next note in a set of pieces selected from that style. To test whether global structure has in fact been been learned, we use the model to generate new pieces of music in that style. In a discussion of the model we highlight its sensitivity to three distinct levels of temporal order in music corresponding to local structure, long-timescale metrical structure and long-timescale non-metrical structure.},
author = {Eck, Douglas and Lapalme, Jasmin},
journal = {University of Montreal, Department of Computer {\ldots}},
number = {1983},
pages = {1--12},
title = {{Learning musical structure directly from sequences of music}},
year = {2008}
}

@article{Hadjeres2016,
abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
archivePrefix = {arXiv},
arxivId = {1612.01010},
author = {Hadjeres, Ga{\"{e}}tan and Pachet, Fran{\c{c}}ois and Nielsen, Frank},
eprint = {1612.01010},
isbn = {4573196480018},
title = {{DeepBach: a Steerable Model for Bach Chorales Generation}},
year = {2016}
}


@article{Liang2017,
abstract = {This paper presents " BachBot " : an end-to-end automatic composition system for composing and completing mu-sic in the style of Bach's chorales using a deep long short-term memory (LSTM) generative model. We pro-pose a new sequential encoding scheme for polyphonic music and a model for both composition and harmoniza-tion which can be efficiently sampled without expensive Markov Chain Monte Carlo (MCMC). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess BachBot's success, we conducted one of the largest musical discrimination tests on 2336 par-ticipants. Among the results, the proportion of responses correctly differentiating BachBot from Bach was only 1{\%} better than random guessing.},
author = {Liang, Feynman and Gotham, Mark and Johnson, Matthew and Shotton, Jamie},
journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference},
pages = {449--456},
title = {{Automatic Stylistic Composition of Bach Chorales with Deep LSTM}},
year = {2017}
}

@article{Raffel2016,
abstract = {Sequences of feature vectors are a natural way of representing temporal data. Given a database of sequences, a fundamental task is to find the database entry which is the most similar to a query. In this thesis, we present learning-based methods for efficiently and accurately comparing sequences in order to facilitate large-scale sequence search. Throughout, we will focus on the problem of matching MIDI files (a digital score format) to a large collection of audio recordings of music. The combination of our proposed approaches enables us to create the largest corpus of paired MIDI files and audio recordings ever assembled.},
author = {Raffel, Colin},
isbn = {9781339928296},
pages = {222},
title = {{Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching}},
year = {2016}
}

@article{Dong2017,
abstract = {Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/ .},
archivePrefix = {arXiv},
arxivId = {1709.06298},
author = {Dong, Hao-Wen and Hsiao, Wen-Yi and Yang, Li-Chia and Yang, Yi-Hsuan},
eprint = {1709.06298},
title = {{MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment}},
year = {2017}
}

@article{Dong2017,
abstract = {Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/ .},
archivePrefix = {arXiv},
arxivId = {1709.06298},
author = {Dong, Hao-Wen and Hsiao, Wen-Yi and Yang, Li-Chia and Yang, Yi-Hsuan},
eprint = {1709.06298},
title = {{MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment}},
year = {2017}
}

@article{Dong2018,
abstract = {It has been shown recently that convolutional generative adversarial networks (GANs) are able to capture the temporal-pitch patterns in music using the piano-roll representation, which represents music by binary-valued time-pitch matrices. However, existing models can only generate real-valued piano-rolls and require further post-processing (e.g. hard thresholding, Bernoulli sampling) at test time to obtain the final binary-valued results. In this work, we first investigate how the real-valued predictions generated by the generator may lead to difficulties in training the discriminator. To overcome the binarization issue, we propose to append to the generator an additional refiner network, which uses binary neurons at the output layer. The whole network can be trained in a two-stage training setting: the generator and the discriminator are pretrained in the first stage; the refiner network is then trained along with the discriminator in the second stage to refine the real-valued piano-rolls generated by the pretrained generator to binary-valued ones. The proposed model is able to directly generate binary-valued piano-rolls at test time. Experimental results show improvements to the existing models in most of the evaluation metrics. All source code, training data and audio samples can be found at https://salu133445.github.io/bmusegan/ .},
archivePrefix = {arXiv},
arxivId = {1804.09399},
author = {Dong, Hao-Wen and Yang, Yi-Hsuan},
doi = {arXiv:1804.09399v2},
eprint = {1804.09399},
title = {{Convolutional Generative Adversarial Networks with Binary Neurons for Polyphonic Music Generation}},
year = {2018}
}

@article{Bretan2016,
abstract = {Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.},
archivePrefix = {arXiv},
arxivId = {1612.03789},
author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
eprint = {1612.03789},
pages = {1--13},
title = {{A Unit Selection Methodology for Music Generation Using Deep Neural Networks}},
year = {2016}
}

@article{Sturm2015,
abstract = {We demonstrate two generative models created by train-ing a recurrent neural network (RNN) with three hidden layers of long short-term memory (LSTM) units. This ex-tends past work in numerous directions, including training deeper models with nearly 24,000 high-level transcriptions of folk tunes. We discuss our on-going work.},
author = {Sturm, Bob L and Santos, Jo{\~{a}}o Felipe and Korshunova, Iryna},
journal = {Ismir},
title = {{Folk Music Style Modelling By Recurrent Neural Networks With Long Short Term Memory Units}},
year = {2015}
}

@article{Madjiheurem2016,
abstract = {In natural language processing, the well-known Skip-gram model learns vector representations of words that carry meaningful syntactic and semantic information. In our work, we investigate whether similar high-quality embeddings can be found for symbolic music data. We introduce three NLP inspired models to learn vector representations of chords and we evaluate their performance. We show that an adaptation of the sequence-to-sequence model is by far superior to the other proposed model.},
author = {Madjiheurem, Sephora and Qu, Lizhen and Walder, Christian},
number = {Nips},
title = {{Chord2Vec: Learning Musical Chord Embeddings}},
year = {2016}
}

@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
isbn = {9781937284961},
issn = {09205691},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
year = {2014}
}

@article{Walder2016,
abstract = {In this paper, we consider the problem of probabilistically modelling symbolic music data. We introduce a representation which reduces polyphonic music to a univariate categorical sequence. In this way, we are able to apply state of the art natural language processing techniques, namely the long short-term memory sequence model. The representation we employ permits arbitrary rhythmic structure, which we assume to be given. We show that our model is effective on four out of four piano roll based benchmark datasets. We further improve our model by augmenting our training data set with transpositions of the original pieces through all musical keys, thereby convincingly advancing the state of the art on these benchmark problems. We also fit models to music which is unconstrained in its rhythmic structure, discuss the properties of this model, and provide musical samples which are more sophisticated than previously possible with this class of recurrent neural network sequence models. We also provide our newly preprocessed data set of non piano-roll music data.},
archivePrefix = {arXiv},
arxivId = {1606.01368},
author = {Walder, Christian},
eprint = {1606.01368},
title = {{Modelling Symbolic Music: Beyond the Piano Roll}},
year = {2016}
}

@article{Ghedini2015,
abstract = {This chapter introduces the vision and the technical challenges of the Flow Machines project. Flow Machines aim at fostering creativity in artistic domains such as music and literature. We first observe that typically, great artists do not output just single artefacts but develop novel, individual styles. Style mirrors an individual's uniqueness; style makes an artist's work recognised and recognisable. Artists develop their own style after prolonged periods of imitation and exploration of the style of others. We envision style exploration as the application of existing styles, considered as texture, to arbitrary constraints, considered as structure. The goal of Flow Machines is to assist this process by allowing users to explicitly manipulate styles as computational objects. During interactions with Flow Machines, the user can create artefacts (melodies, texts, orchestrations) by combining styles with arbitrary constraints. Style exploration under user-defined constraints raises complex sequence generation issues that were addressed and solved for the most part during the first half of the project. We illustrate the potential of these techniques for style exploration with three examples.},
author = {Ghedini, Fiammetta and Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1007/978-981-287-618-8_18},
isbn = {9789812876188},
journal = {Multidisciplinary Contributions to the Science of Creative Thinking},
pages = {325--343},
title = {{Creating music and texts with flow machines}},
year = {2015}
}